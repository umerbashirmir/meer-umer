{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTFSJNuBWRqc"
      },
      "outputs": [],
      "source": [
        "THEORETICAL QUESTIONS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble Learning Questions\n",
        "#1. Can we use Bagging for regression problems?\n",
        "Answer: Yes, Bagging can be used for regression problems. The BaggingRegressor in scikit-learn is an example. It reduces variance by averaging predictions from multiple base regressors, typically Decision Trees.\n",
        "\n",
        "\n",
        "#2. What is the difference between multiple model training and single model training?\n",
        "Answer: Single model training: One model is trained on the dataset and used for predictions.\n",
        "Multiple model training: Multiple models are trained, and their outputs are combined to improve accuracy and reduce overfitting, as seen in ensemble methods like Bagging and Boosting.\n",
        "\n",
        "\n",
        "#3. Explain the concept of feature randomness in Random Forest.\n",
        "Answer:Random Forest introduces feature randomness by selecting a random subset of features at each split in a Decision Tree. This reduces correlation between trees and enhances generalization.\n",
        "\n",
        "#4. What is OOB (Out-of-Bag) Score?\n",
        "Answer:OOB Score is an estimate of model accuracy calculated using data samples not included in training due to bootstrap sampling in Bagging-based methods like Random Forest.\n",
        "\n",
        "#5. How can you measure the importance of features in a Random Forest model?\n",
        "Answer:Feature importance is measured using metrics like:\n",
        "Mean Decrease in Impurity (MDI) – Measures how much each feature reduces impurity across splits.\n",
        "Permutation Importance – Measures the change in model performance when a feature's values are shuffled.\n",
        "\n",
        "#6. Explain the working principle of a Bagging Classifier.\n",
        "Answer:A Bagging Classifier trains multiple weak learners (e.g., Decision Trees) on different bootstrap samples and aggregates their predictions through majority voting (classification) or averaging (regression).\n",
        "\n",
        "#7. How do you evaluate a Bagging Classifier’s performance?\n",
        "Answer: Performance is evaluated using:\n",
        "Accuracy (for classification tasks)\n",
        "Precision, Recall, F1-score (for imbalanced datasets)\n",
        "ROC-AUC Score (for probabilistic outputs)\n",
        "\n",
        "\n",
        "#8. How does a Bagging Regressor work?\n",
        "Answer: A Bagging Regressor trains multiple base regressors (e.g., Decision Trees) on bootstrap samples and averages their predictions to reduce variance and improve generalization.\n",
        "\n",
        "\n",
        "#9. What is the main advantage of ensemble techniques?\n",
        "Answer:Ensemble techniques improve accuracy, reduce overfitting, and provide robustness by combining multiple models instead of relying on a single weak learner.\n",
        "\n",
        "\n",
        "#10. What is the main challenge of ensemble methods?\n",
        "Answer: The main challenge is increased computational complexity, as multiple models must be trained, requiring more time and resources.\n",
        "\n",
        "#11. Explain the key idea behind ensemble techniques.\n",
        "Answer: Ensemble techniques combine predictions from multiple models to improve accuracy and stability, leveraging diversity among base learners.\n",
        "\n",
        "#12. What is a Random Forest Classifier?\n",
        "Answer: A Random Forest Classifier is an ensemble model that builds multiple Decision Trees and aggregates their predictions using majority voting.\n",
        "\n",
        "#13. What are the main types of ensemble techniques?\n",
        "Answer:The main types include:\n",
        "\n",
        "Bagging – Reduces variance by averaging multiple model outputs.\n",
        "Boosting – Reduces bias by training models sequentially.\n",
        "Stacking – Combines multiple models using a meta-learner.\n",
        "#14. What is ensemble learning in machine learning?\n",
        "Answer: Ensemble learning is a technique that combines multiple models to improve predictive performance beyond what a single model can achieve.\n",
        "\n",
        "#15. When should we avoid using ensemble methods?\n",
        "Answer: Ensemble methods should be avoided when:\n",
        "\n",
        "Computational resources are limited.\n",
        "The dataset is small, leading to overfitting.\n",
        "A single model already performs well.\n",
        "#16. How does Bagging help in reducing overfitting?\n",
        "Answer: Bagging reduces overfitting by training multiple models on different bootstrap samples and averaging their outputs, preventing any single model from learning noise.\n",
        "\n",
        "#17. Why is Random Forest better than a single Decision Tree?\n",
        "Answer: Random Forest reduces overfitting by averaging multiple trees, leading to better generalization and stability compared to a single Decision Tree.\n",
        "\n",
        "#18. What is the role of bootstrap sampling in Bagging?\n",
        "Answer: Bootstrap sampling creates diverse training datasets by randomly selecting samples with replacement, ensuring models learn different aspects of the data.\n",
        "\n",
        "#19. What are some real-world applications of ensemble techniques?\n",
        "Answer:\n",
        "Finance: Fraud detection\n",
        "Healthcare: Disease diagnosis\n",
        "E-commerce: Recommendation systems\n",
        "Self-driving cars: Object detection\n",
        "#20. What is the difference between Bagging and Boosting?\n",
        "Answer:Bagging trains multiple models independently and averages their predictions.\n",
        "Boosting trains models sequentially, where each model corrects errors from the previous one."
      ],
      "metadata": {
        "id": "5iaC7hHhWSts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zDj_abecBuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "go7MIOtvWSwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and MSE\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Bagging Regressor MSE:\", mse)\n",
        "\n",
        "\n",
        "\n",
        "#Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "# Feature Importance\n",
        "importances = rf_clf.feature_importances_\n",
        "for i, v in enumerate(importances):\n",
        "    print(f\"Feature {i}: Importance {v:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "#Q24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "tree_pred = tree_reg.predict(X_test)\n",
        "tree_mse = mean_squared_error(y_test, tree_pred)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Decision Tree MSE:\", tree_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n",
        "\n",
        "\n",
        "#Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf_oob.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", rf_clf_oob.oob_score_)\n",
        "Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier with SVM Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "#Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "    print(f\"Random Forest with {n} trees: Accuracy {acc}\")\n",
        "\n",
        "\n",
        "\n",
        "#Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "bagging_lr = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"Bagging Classifier with Logistic Regression AUC Score:\", roc_auc)\n",
        "\n",
        "\n",
        "#Q29. Train a Random Forest Regressor and analyze feature importance scores.\n",
        "\n",
        "importances = rf_reg.feature_importances_\n",
        "for i, v in enumerate(importances):\n",
        "    print(f\"Feature {i}: Importance {v:.4f}\")\n",
        "\n",
        "\n",
        "#Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "bagging_acc = accuracy_score(y_test, bagging_clf.predict(X_test))\n",
        "rf_acc = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Classifier Accuracy:\", rf_acc)\n",
        "\n",
        "\n",
        " #31Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for n in [5, 10, 20, 50]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
        "    print(f\"Bagging Regressor with {n} estimators - MSE: {mse}\")\n",
        "\n",
        "\n",
        "#33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "misclassified = np.where(y_pred != y_test)[0]\n",
        "print(\"Misclassified samples:\", misclassified)\n",
        "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt_clf.predict(X_test))\n",
        "\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_acc = accuracy_score(y_test, bagging_clf.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
        "\n",
        "\n",
        "#35. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('svm', SVC(probability=True)),\n",
        "        ('lr', LogisticRegression())\n",
        "    ],\n",
        "    final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "stacking_acc = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_acc)\n",
        "#37. Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "sorted_indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"Feature {sorted_indices[i]}: Importance {importances[sorted_indices[i]]:.4f}\")\n",
        "\n",
        "\n",
        "#38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "for depth in [5, 10, 15, None]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "    print(f\"Random Forest with max_depth={depth}: Accuracy {acc}\")\n",
        "\n",
        "\n",
        "#40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "base_estimators = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(),\n",
        "    \"KNeighbors\": KNeighborsRegressor()}\n",
        "\n",
        "for name, estimator in base_estimators.items():\n",
        "    bagging_reg = BaggingRegressor(base_estimator=estimator, n_estimators=10, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
        "    print(f\"{name} as base estimator - MSE: {mse}\")\n",
        "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#42. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "print(\"Cross-validation Accuracy:\", scores.mean())\n",
        "\n",
        "\n",
        "\n",
        "#43. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n",
        "#44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier()),\n",
        "        ('lr', LogisticRegression())\n",
        "    ],\n",
        "    final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "stacking_acc = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_acc)\n",
        "#45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "for bootstrap in [True, False]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, bootstrap=bootstrap, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
        "    print(f\"Bagging Regressor with bootstrap={bootstrap} - MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "_IduylPrWSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQi2rxRaWS1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVrgJojjWS4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlRaYcXfWS7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tXVdZVoWS-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqb0DVixWTBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CJz9jcvWTD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gqnksoAvWTHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDu7wN7MWTKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ORrHFVh8WTNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfnLErJtWTQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EheltWyWTT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-5jx0ypWTWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7hLxp5qWTZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pLClL0sWTch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tk7E1r1YWTfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXVC9UStWTh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-iejXrFWTnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w26LeCzxWTs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Giwpdnj6WTvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PvsLXbSWTyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqoTzhjIWT10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ed4qbtg7WT4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R40ZgzUNWT7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyj75UdXWT-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZPdLVa0CWUBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uexhypZPWUEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0n1ZC0DWUHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwadJBwbWUKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X531FbtJWUM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ciFalfFWUP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gA-2MlOgWUS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFiCcu6fWUVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5klDC9-1WUYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRrv_DYrWUbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5RSjpWXWUeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPZgBRd7WUhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wdrej2SxWUkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4EjvVTAWUnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yUKRIDJfWUqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzlMUpyiWUso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DpNXtKYvWUvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIppqF-QWUx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1s7SUNnWU0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8U0seFW9WU3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-B5Xxwa1WU6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gxG-mPkWU91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}