{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-L9dn8ey7xK"
      },
      "outputs": [],
      "source": [
        "#1 What is Boosting in Machine Learning\n",
        "\n",
        "#Answer.Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (typically decision trees)\n",
        " to create a strong learner. It works by sequentially training models, with each new model focusing on correcting the errors made by\n",
        " the previous ones.\n",
        "\n",
        "### **How Boosting Works:**\n",
        "1. **Initialize Weights:** Each training sample is assigned an initial weight.\n",
        "2. **Train Weak Learner:** A weak model (e.g., a shallow decision tree) is trained on the dataset.\n",
        "3. **Update Weights:** Samples that were misclassified are given higher weights so that the next weak learner focuses more on them.\n",
        "4. **Combine Learners:** The final model is a weighted sum of all weak learners, where stronger models get higher importance.\n",
        "\n",
        "### **Popular Boosting Algorithms:**\n",
        "- **AdaBoost (Adaptive Boosting)** â€“ Adjusts sample weights iteratively.\n",
        "- **Gradient Boosting (GBM)** â€“ Minimizes loss function using gradient descent.\n",
        "- **XGBoost (Extreme Gradient Boosting)** â€“ An optimized version of GBM with speed and efficiency improvements.\n",
        "- **LightGBM** â€“ Uses histogram-based learning for faster training.\n",
        "- **CatBoost** â€“ Designed for categorical data handling.\n",
        "\n",
        "Boosting is widely used in real-world applications like fraud detection, recommendation systems, and predictive modeling because of its\n",
        " ability to improve accuracy while reducing overfitting when tuned correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4 How does Boosting differ from Bagging\n",
        "#Answer.Boosting and Bagging are both **ensemble learning techniques**, but they differ in how they build and combine multiple models.\n",
        "\n",
        "\n",
        "### **Key Takeaways:**\n",
        "- **Bagging** is useful when the main problem is **high variance**, as it stabilizes predictions (e.g., Random Forest).\n",
        "- **Boosting** is useful when the main problem is **high bias**, as it creates a strong learner from weak models (e.g., XGBoost, AdaBoost).\n"
      ],
      "metadata": {
        "id": "76a67kUozSZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 What is the key idea behind AdaBoost\n",
        "#answer.### **Key Idea Behind AdaBoost (Adaptive Boosting)**\n",
        "The **core idea of AdaBoost** is to combine multiple weak learners (typically shallow decision trees or stumps) into a single\n",
        "strong classifier by focusing more on misclassified instances at each step.\n",
        "\n",
        "### **How AdaBoost Works:**\n",
        "1. **Initialize Weights:**\n",
        "   - Assign equal weights to all training samples.\n",
        "\n",
        "2. **Train Weak Learner:**\n",
        "   - A weak model (often a decision stump) is trained on the dataset.\n",
        "\n",
        "3. **Compute Error & Update Weights:**\n",
        "   - If a sample is misclassified, increase its weight (so the next model pays more attention to it).\n",
        "   - If correctly classified, decrease its weight.\n",
        "\n",
        "4. **Repeat for Multiple Weak Learners:**\n",
        "   - Each new weak learner focuses more on the previously misclassified samples.\n",
        "   - Assign a weight to each weak learner based on its accuracy.\n",
        "\n",
        "5. **Final Prediction:**\n",
        "   - Combine all weak learners into a weighted sum (majority vote for classification, weighted sum for regression).\n",
        "\n",
        "### **Why AdaBoost Works Well:**\n",
        "âœ… **Focuses on hard-to-classify samples**\n",
        "âœ… **Reduces bias by sequential learning**\n",
        "âœ… **Less prone to overfitting compared to some other boosting methods**\n",
        "\n",
        "### **Mathematical Intuition:**\n",
        "- Each weak classifier is assigned a weight:\n",
        "  \\[\n",
        "  \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - e_t}{e_t} \\right)\n",
        "  \\]\n",
        "  where \\( e_t \\) is the error of the weak learner.\n",
        "- Final prediction is a weighted sum of all weak learners.\n"
      ],
      "metadata": {
        "id": "rqBG6llny-wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4Explain the working of AdaBoost with an example\n",
        "\n",
        "\n",
        "#Anser.### **Working of AdaBoost with an Example**\n",
        "AdaBoost (Adaptive Boosting) is an ensemble learning method that improves weak classifiers by focusing on misclassified instances\n",
        "in each iteration.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation of AdaBoost**\n",
        "Letâ€™s take a **binary classification example**:\n",
        "We have a dataset where we classify whether a person will buy a product based on age.\n",
        "\n",
        "| Age  | Buys Product (Yes=1, No=0) |\n",
        "|------|----------------------------|\n",
        "| 25   | 0                          |\n",
        "| 30   | 0                          |\n",
        "| 35   | 1                          |\n",
        "| 40   | 1                          |\n",
        "| 45   | 1                          |\n",
        "\n",
        "We'll use **decision stumps** (one-level decision trees) as weak learners.\n",
        "\n",
        "#### **Step 1: Initialize Weights**\n",
        "- Assign **equal weights** to all samples:\n",
        "  \\[\n",
        "  w_i = \\frac{1}{N} = \\frac{1}{5} = 0.2\n",
        "  \\]\n",
        "\n",
        "#### **Step 2: Train First Weak Learner**\n",
        "- A simple decision stump splits at **Age < 35**:\n",
        "  - Predict **0** if Age < 35\n",
        "  - Predict **1** if Age â‰¥ 35\n",
        "- This model misclassifies **two samples** (Age = 25, 30).\n",
        "\n",
        "#### **Step 3: Compute Error**\n",
        "- Weighted error \\( e_1 \\):\n",
        "  \\[\n",
        "  e_1 = \\sum \\text{(weight of misclassified samples)}\n",
        "  \\]\n",
        "  Here, two misclassified points contribute to error:\n",
        "  \\[\n",
        "  e_1 = 0.2 + 0.2 = 0.4\n",
        "  \\]\n",
        "\n",
        "#### **Step 4: Compute Model Weight**\n",
        "- Compute importance of the weak learner:\n",
        "  \\[\n",
        "  \\alpha_1 = \\frac{1}{2} \\ln \\left( \\frac{1 - e_1}{e_1} \\right) = \\frac{1}{2} \\ln \\left( \\frac{1 - 0.4}{0.4} \\right) = 0.2\n",
        "  \\]\n",
        "\n",
        "#### **Step 5: Update Sample Weights**\n",
        "- Increase the weights of misclassified samples so that the next model focuses more on them.\n",
        "- New weights:\n",
        "  \\[\n",
        "  w_{\\text{new}} = w_{\\text{old}} \\times e^{\\alpha}\n",
        "  \\]\n",
        "\n",
        "#### **Step 6: Train the Next Weak Learner**\n",
        "- A new decision stump is trained, focusing more on misclassified points.\n",
        "- This process **repeats** for multiple iterations.\n",
        "\n",
        "#### **Final Prediction**\n",
        "- Combine all weak classifiers using their weights.\n",
        "- Use a weighted **majority vote** for final classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Implementation of AdaBoost**\n",
        "Here's a simple implementation using `sklearn`:\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate toy dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create AdaBoost model with decision stumps as weak learners\n",
        "model = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "âœ… **Sequential Learning** â€“ Each weak learner improves on previous mistakes.\n",
        "âœ… **Misclassified samples get higher weight** â€“ Forces model to focus on harder examples.\n",
        "âœ… **Final model is a weighted sum of weak learners** â€“ Stronger models get more influence.\n"
      ],
      "metadata": {
        "id": "CeH-B0Vgy-s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 What is Gradient Boosting, and how is it different from AdaBoost\n",
        "\n",
        "#Answer. ### **What is Gradient Boosting?**\n",
        "Gradient Boosting is an ensemble learning technique that builds models sequentially, just like AdaBoost, but instead of\n",
        "focusing on misclassified samples, it minimizes the loss function using **gradient descent**.\n",
        "\n",
        "ðŸ‘‰ **Key Idea:**\n",
        "Each new weak learner (typically a decision tree) is trained to correct the residual errors (difference between actual\n",
        "    and predicted values) of the previous models.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Gradient Boosting Works (Step-by-Step)**\n",
        "1. **Initialize Model:**\n",
        "   - Start with a simple model (e.g., predicting the average value for regression).\n",
        "\n",
        "2. **Compute Residuals:**\n",
        "   - Calculate the difference between actual values and predicted values (these are the \"errors\" we want to fix).\n",
        "\n",
        "3. **Train a Weak Learner (Decision Tree) on Residuals:**\n",
        "   - A new decision tree is trained to predict the residual errors.\n",
        "\n",
        "4. **Update the Model:**\n",
        "   - Adjust predictions by adding the weighted contribution of the new tree:\n",
        "     \\[\n",
        "     F_{new} = F_{old} + \\lambda \\cdot h(x)\n",
        "     \\]\n",
        "     where \\( \\lambda \\) is a learning rate and \\( h(x) \\) is the new tree.\n",
        "\n",
        "5. **Repeat Steps 2â€“4 for Multiple Iterations**\n",
        "   - Keep adding new models that minimize the loss function until convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between Gradient Boosting & AdaBoost**\n",
        "| Feature             | **Gradient Boosting**                                   | **AdaBoost**                                        |\n",
        "|---------------------|------------------------------------------------------|--------------------------------------------------|\n",
        "| **Error Handling**  | Minimizes the **gradient of the loss function**       | Focuses on misclassified samples by adjusting weights |\n",
        "| **Loss Function**   | Uses **gradient descent** to reduce loss              | Uses an **exponential loss function**               |\n",
        "| **Boosting Method** | Corrects **residual errors** iteratively              | Adjusts **sample weights** iteratively              |\n",
        "| **Performance**     | More flexible, can optimize any differentiable loss   | Works well with classification problems            |\n",
        "| **Overfitting**     | More prone if not regularized (needs tuning)          | Less prone to overfitting with proper hyperparameters |\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Implementation of Gradient Boosting**\n",
        "Hereâ€™s an example using `sklearn`:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Model\n",
        "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use AdaBoost vs. Gradient Boosting?**\n",
        "- **Use AdaBoost** when you have **weak learners** (e.g., decision stumps) and need to **focus on misclassified samples**.\n",
        "- **Use Gradient Boosting** when you need **more flexibility** and can optimize a **custom loss function** (e.g., regression problems).\n"
      ],
      "metadata": {
        "id": "XPAY4Nguy-qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. What is the loss function in Gradient Boosting\n",
        "\n",
        "#Ans . ### **Loss Function in Gradient Boosting**\n",
        "Gradient Boosting minimizes a **loss function** by using **gradient descent** to sequentially add weak learners (decision trees)\n",
        " that correct previous errors. The choice of the loss function depends on the type of problem:\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Loss Functions in Gradient Boosting**\n",
        "#### **1. Regression Problems**\n",
        "For predicting continuous values (e.g., house prices):\n",
        "- **Mean Squared Error (MSE)**:\n",
        "  \\[\n",
        "  L(y, \\hat{y}) = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "  \\]\n",
        "  - Most commonly used for regression.\n",
        "  - The gradient (derivative) is simply the **negative residuals**:\n",
        "    \\[\n",
        "    \\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})\n",
        "    \\]\n",
        "  - The model learns to correct predictions by reducing these residuals.\n",
        "\n",
        "- **Mean Absolute Error (MAE)**:\n",
        "  \\[\n",
        "  L(y, \\hat{y}) = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|\n",
        "  \\]\n",
        "  - Less sensitive to outliers compared to MSE.\n",
        "\n",
        "#### **2. Classification Problems**\n",
        "For predicting categories (e.g., spam vs. not spam):\n",
        "- **Log Loss (Binary Classification)**:\n",
        "  \\[\n",
        "  L(y, \\hat{y}) = - \\sum y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})\n",
        "  \\]\n",
        "  - Used for binary classification.\n",
        "  - The model outputs probabilities, and log loss penalizes incorrect confidence.\n",
        "\n",
        "- **Multiclass Log Loss (Cross-Entropy Loss)**:\n",
        "  \\[\n",
        "  L(y, \\hat{y}) = - \\sum y_k \\log(\\hat{y}_k)\n",
        "  \\]\n",
        "  - Used when there are **more than two** classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Gradient Boosting Uses the Loss Function**\n",
        "1. **Compute Residuals (Negative Gradient of Loss Function)**\n",
        "   - Instead of directly predicting \\( y \\), each tree predicts the residuals (errors) of the previous model.\n",
        "   - For MSE:\n",
        "     \\[\n",
        "     r_i = y_i - \\hat{y}_i\n",
        "     \\]\n",
        "   - For Log Loss:\n",
        "     \\[\n",
        "     r_i = y - \\sigma(\\hat{y})\n",
        "     \\]\n",
        "     where \\( \\sigma(\\hat{y}) \\) is the sigmoid function.\n",
        "\n",
        "2. **Fit Weak Learner to Residuals**\n",
        "   - A decision tree is trained to predict the residuals.\n",
        "\n",
        "3. **Update Predictions**\n",
        "   - The new model is added with a learning rate \\( \\lambda \\):\n",
        "     \\[\n",
        "     F_{new}(x) = F_{old}(x) + \\lambda \\cdot h(x)\n",
        "     \\]\n",
        "     where \\( h(x) \\) is the weak learner.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "âœ… **Choice of Loss Function Depends on Task** (Regression â†’ MSE, Classification â†’ Log Loss).\n",
        "âœ… **Gradient Boosting Minimizes Loss Using Gradient Descent.**\n",
        "âœ… **Trees Predict Residuals (Errors) Instead of Direct Values.**\n"
      ],
      "metadata": {
        "id": "InKKm5Xby-nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. How does XGBoost improve over traditional Gradient Boosting\n",
        "\n",
        "#Answer. XGBoost (**eXtreme Gradient Boosting**) improves upon traditional Gradient Boosting in several ways, making it **faster,\n",
        "more efficient, and less prone to overfitting**. Hereâ€™s how:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Regularization to Prevent Overfitting**\n",
        "Traditional Gradient Boosting doesnâ€™t include built-in regularization, making it prone to overfitting. XGBoost introduces:\n",
        "âœ… **L1 Regularization (Lasso Regression)**: Encourages sparsity in the model.\n",
        "âœ… **L2 Regularization (Ridge Regression)**: Prevents large coefficients.\n",
        "\n",
        "ðŸ”¹ **Objective function in XGBoost:**\n",
        "\\[\n",
        "\\text{Objective} = \\sum L(y, \\hat{y}) + \\lambda ||w||_1 + \\alpha ||w||_2^2\n",
        "\\]\n",
        "where \\( L(y, \\hat{y}) \\) is the loss function, and \\( \\lambda, \\alpha \\) control regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Second-Order Approximation for Faster Convergence**\n",
        "Traditional Gradient Boosting optimizes using only the first derivative (gradient). XGBoost improves this by using\n",
        "**both first and second derivatives (Hessian matrix)** for better optimization.\n",
        "âœ… **More accurate weight updates**\n",
        "âœ… **Faster convergence**\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Efficient Tree Splitting (Histogram-based Algorithm)**\n",
        "Instead of checking every possible split, XGBoost:\n",
        "âœ… **Uses histogram-based binning** â†’ Reduces complexity.\n",
        "âœ… **Uses Weighted Quantile Sketch** â†’ Handles large datasets efficiently.\n",
        "\n",
        "ðŸ’¡ **Result**: Faster and more memory-efficient training.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Parallel Processing & Optimized Tree Construction**\n",
        "Traditional Gradient Boosting builds trees **sequentially**, while XGBoost:\n",
        "âœ… **Parallelizes feature selection** during tree construction.\n",
        "âœ… **Uses cache-aware block structures** for CPU/GPU acceleration.\n",
        "âœ… **Much faster training** compared to traditional Gradient Boosting.\n",
        "\n",
        "ðŸš€ **Result**: XGBoost can run on large datasets **10x faster** than regular Gradient Boosting.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Handling Missing Values Automatically**\n",
        "Traditional Gradient Boosting requires missing values to be manually imputed. XGBoost:\n",
        "âœ… **Automatically learns the best default direction** for missing values during training.\n",
        "âœ… **More robust to real-world messy datasets.**\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Early Stopping for Faster Training**\n",
        "âœ… **Stops training when validation error stops improving.**\n",
        "âœ… **Prevents overfitting & reduces training time.**\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Column & Row Subsampling for Better Generalization**\n",
        "Traditional Gradient Boosting uses all features at every split. XGBoost:\n",
        "âœ… **Randomly selects a subset of features for each tree (like Random Forest).**\n",
        "âœ… **Prevents overfitting and reduces variance.**\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison: XGBoost vs. Traditional Gradient Boosting**\n",
        "| Feature                   | **Traditional Gradient Boosting** | **XGBoost** |\n",
        "|---------------------------|---------------------------------|-------------|\n",
        "| **Regularization**        | No regularization              | L1 & L2 regularization |\n",
        "| **Optimization**          | Uses only first-order gradient | Uses second-order gradients |\n",
        "| **Tree Splitting**        | Greedy, slow                   | Histogram-based, fast |\n",
        "| **Parallel Processing**   | No                              | Yes |\n",
        "| **Missing Values Handling** | Manual                        | Automatic |\n",
        "| **Early Stopping**        | No                              | Yes |\n",
        "| **Feature Subsampling**   | No                              | Yes |\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Implementation of XGBoost**\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Takeaways**\n",
        "âœ… **XGBoost is faster and more scalable than traditional Gradient Boosting.**\n",
        "âœ… **Regularization (L1 & L2) reduces overfitting.**\n",
        "âœ… **Parallelization + histogram-based splitting speeds up training.**\n",
        "âœ… **Automatically handles missing values.**\n"
      ],
      "metadata": {
        "id": "3CT6QhWDy-kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 What is the difference between XGBoost and CatBoost\n",
        "\n",
        "#Answer. ### **XGBoost vs. CatBoost: Key Differences** ðŸš€ðŸ±\n",
        "Both XGBoost and CatBoost are powerful gradient boosting algorithms, but they have **different strengths**.\n",
        " Hereâ€™s a breakdown of how they differ:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Handling Categorical Features**\n",
        "| Feature | **XGBoost** | **CatBoost** |\n",
        "|---------|-----------|------------|\n",
        "| **Categorical Data Support** | Does **not** natively support categorical features; requires one-hot encoding or label encoding. |\n",
        "                                                                              **Natively supports categorical data** (no need for encoding). |\n",
        "| **Encoding Method** | Uses **one-hot encoding** or manual transformation. | Uses **\"ordered boosting\" + target-based encoding**,\n",
        "                                                                                                  which reduces overfitting. |\n",
        "| **Performance on Categorical Data** | Slower and can suffer from overfitting when many categories exist. |\n",
        "                                                                            **Faster & more accurate** when handling categorical features. |\n",
        "\n",
        "âœ” **CatBoost wins** for datasets with many categorical variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Speed & Training Efficiency**\n",
        "| Feature | **XGBoost** | **CatBoost** |\n",
        "|---------|-----------|------------|\n",
        "| **Training Speed** | Fast but requires careful hyperparameter tuning. | **Optimized for fast training with automatic tuning.** |\n",
        "| **Tree Growth** | **Level-wise** (splits all nodes at the same depth before moving deeper). | **Oblivious (symmetric) trees**, where\n",
        "                                                                                                all nodes at a level split on the same feature. |\n",
        "| **Parallelization** | Supports **GPU acceleration**. | More optimized parallelization, often **faster on GPUs** than XGBoost. |\n",
        "\n",
        "âœ” **CatBoost is faster** due to better tree-building and optimized parallelism.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Handling Missing Values**\n",
        "| Feature | **XGBoost** | **CatBoost** |\n",
        "|---------|-----------|------------|\n",
        "| **Missing Value Handling** | Uses default direction for missing values. | **Automatically handles missing values**\n",
        "and learns the best split strategy. |\n",
        "\n",
        "âœ” **CatBoost wins** for datasets with missing values.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Overfitting Prevention**\n",
        "| Feature | **XGBoost** | **CatBoost** |\n",
        "|---------|-----------|------------|\n",
        "| **Regularization** | L1 & L2 regularization + early stopping. | **Ordered boosting** (reduces target leakage) + L2 regularization. |\n",
        "| **Default Hyperparameters** | Requires tuning for best performance. | More optimized **out-of-the-box**. |\n",
        "\n",
        "âœ” **CatBoost requires less tuning** and avoids overfitting better.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Performance on Different Data Types**\n",
        "| Data Type | **XGBoost** | **CatBoost** |\n",
        "|-----------|-----------|------------|\n",
        "| **Numerical Data** | Performs well with proper feature engineering. | Performs well, but no major advantage over XGBoost. |\n",
        "| **Categorical Data** | Requires preprocessing (one-hot encoding). | **Best for categorical data** (automated handling). |\n",
        "| **Imbalanced Data** | Needs manual tweaking (e.g., class weights). | Performs well without much tuning. |\n",
        "\n",
        "âœ” **XGBoost is great for numerical data**, but **CatBoost is better for categorical and imbalanced datasets**.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Hyperparameter Tuning Complexity**\n",
        "| Feature | **XGBoost** | **CatBoost** |\n",
        "|---------|-----------|------------|\n",
        "| **Ease of Use** | Requires extensive hyperparameter tuning. | Works well with default parameters. |\n",
        "| **Parameter Complexity** | More complex (learning rate, tree depth, etc.). | Simpler tuning (automatic feature selection). |\n",
        "\n",
        "âœ” **CatBoost is more user-friendly** with fewer hyperparameter adjustments.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. When to Use Which?**\n",
        "| Scenario | **Best Choice** |\n",
        "|----------|---------------|\n",
        "| **Numerical Features & Large Datasets** | âœ… **XGBoost** |\n",
        "| **Many Categorical Features** | âœ… **CatBoost** |\n",
        "| **Need for Fast Training with Less Tuning** | âœ… **CatBoost** |\n",
        "| **Customizable Loss Functions & Fine-Tuning** | âœ… **XGBoost** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example: XGBoost vs. CatBoost in Python**\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# XGBoost Model\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# CatBoost Model\n",
        "cat_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0)\n",
        "cat_model.fit(X_train, y_train)\n",
        "cat_pred = cat_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
        "print(f\"CatBoost Accuracy: {accuracy_score(y_test, cat_pred):.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Verdict**\n",
        "âœ… **Use XGBoost for numerical-heavy datasets** that require fine-tuning.\n",
        "âœ… **Use CatBoost for categorical-heavy datasets** or when you want **less hyperparameter tuning** and **faster training**.\n"
      ],
      "metadata": {
        "id": "rlZ1-3RJy-he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9 What are some real-world applications of Boosting techniques\n",
        "\n",
        "#Answer. ### **Real-World Applications of Boosting Techniques** ðŸš€\n",
        "\n",
        "Boosting algorithms like **AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM, and CatBoost** are widely used in\n",
        "real-world applications due to their high accuracy and ability to handle complex datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Fraud Detection ðŸ•µï¸â€â™‚ï¸**\n",
        "âœ… **Use Case**: Identifying fraudulent transactions in banking & e-commerce.\n",
        "âœ… **Boosting Model**: XGBoost, CatBoost.\n",
        "âœ… **Why?**\n",
        "- Boosting can detect subtle patterns in transaction data.\n",
        "- Works well with imbalanced datasets (fraud cases are rare).\n",
        "\n",
        "ðŸ“Œ **Example**: PayPal and major banks use boosting to detect credit card fraud in real-time.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Healthcare & Medical Diagnosis ðŸ¥**\n",
        "âœ… **Use Case**: Predicting diseases and patient outcomes.\n",
        "âœ… **Boosting Model**: Gradient Boosting, XGBoost, LightGBM.\n",
        "âœ… **Why?**\n",
        "- Handles missing data in medical records.\n",
        "- Improves diagnostic accuracy compared to traditional ML models.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- **Cancer detection** using medical imaging and patient data.\n",
        "- **Predicting heart disease** based on patient symptoms.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Customer Churn Prediction ðŸ“‰**\n",
        "âœ… **Use Case**: Predicting which customers will stop using a service.\n",
        "âœ… **Boosting Model**: XGBoost, CatBoost.\n",
        "âœ… **Why?**\n",
        "- Boosting identifies at-risk customers by analyzing behavioral patterns.\n",
        "- Helps businesses retain customers with targeted marketing.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Telecom companies (Verizon, AT&T) use boosting to **prevent customer churn**.\n",
        "- Netflix predicts **which users will cancel subscriptions**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Recommendation Systems ðŸŽ¯**\n",
        "âœ… **Use Case**: Personalized recommendations in e-commerce & streaming services.\n",
        "âœ… **Boosting Model**: XGBoost, LightGBM.\n",
        "âœ… **Why?**\n",
        "- Captures complex interactions between user preferences and product features.\n",
        "- Improves recommendation accuracy.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Amazon suggests **products** based on your browsing history.\n",
        "- Netflix & YouTube recommend **movies & videos**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Financial Market Prediction ðŸ“Š**\n",
        "âœ… **Use Case**: Predicting stock prices, credit scoring, and risk assessment.\n",
        "âœ… **Boosting Model**: XGBoost, LightGBM.\n",
        "âœ… **Why?**\n",
        "- Boosting models detect **hidden patterns in financial data**.\n",
        "- Used for **credit risk analysis & loan approval**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Hedge funds use XGBoost for **algorithmic trading**.\n",
        "- Banks use CatBoost for **loan approval & risk assessment**.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. NLP & Sentiment Analysis ðŸ’¬**\n",
        "âœ… **Use Case**: Analyzing customer reviews, social media, and chatbot responses.\n",
        "âœ… **Boosting Model**: XGBoost, CatBoost.\n",
        "âœ… **Why?**\n",
        "- Boosting models classify sentiment (positive, negative, neutral).\n",
        "- Used in **chatbots & virtual assistants**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Twitter & Facebook use boosting for **hate speech detection**.\n",
        "- Companies analyze **customer feedback** to improve products.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Autonomous Vehicles ðŸš—**\n",
        "âœ… **Use Case**: Object detection and decision-making in self-driving cars.\n",
        "âœ… **Boosting Model**: XGBoost, LightGBM.\n",
        "âœ… **Why?**\n",
        "- Enhances **computer vision models** for real-time decision-making.\n",
        "- Helps detect **pedestrians, obstacles, and traffic signs**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Tesla & Waymo use **boosting with deep learning** in autonomous driving.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Cybersecurity & Malware Detection ðŸ”’**\n",
        "âœ… **Use Case**: Detecting network intrusions and malicious activities.\n",
        "âœ… **Boosting Model**: XGBoost, Gradient Boosting.\n",
        "âœ… **Why?**\n",
        "- Boosting models identify **unusual network behavior**.\n",
        "- Helps in **spam filtering & phishing detection**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Google uses **boosting for spam detection** in Gmail.\n",
        "- Cybersecurity firms use boosting for **malware classification**.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Insurance Risk Assessment ðŸ“œ**\n",
        "âœ… **Use Case**: Predicting insurance claims and policy fraud.\n",
        "âœ… **Boosting Model**: CatBoost, LightGBM.\n",
        "âœ… **Why?**\n",
        "- Boosting models analyze **historical claim data**.\n",
        "- Helps insurance companies **adjust premiums**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Car insurance companies use boosting to **predict accident risks**.\n",
        "- Health insurance firms use boosting to **detect fraudulent claims**.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Energy & Smart Grid Optimization âš¡**\n",
        "âœ… **Use Case**: Predicting electricity demand and optimizing energy distribution.\n",
        "âœ… **Boosting Model**: LightGBM, XGBoost.\n",
        "âœ… **Why?**\n",
        "- Boosting models forecast **power consumption trends**.\n",
        "- Helps energy companies optimize **grid efficiency**.\n",
        "\n",
        "ðŸ“Œ **Example**:\n",
        "- Smart grids use boosting for **demand forecasting & power outage predictions**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts**\n",
        "âœ… Boosting is used **across industries**, from **finance** to **healthcare**, **cybersecurity**, and **autonomous systems**.\n",
        "âœ… **XGBoost, CatBoost, and LightGBM** dominate Kaggle competitions due to their high accuracy.\n",
        "âœ… Boosting helps businesses **make data-driven decisions faster**.\n"
      ],
      "metadata": {
        "id": "xVAo2xHxy-ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 How does regularization help in XGBoost\n",
        "\n",
        "#Anser. ### **How Regularization Helps in XGBoost** ðŸš€\n",
        "\n",
        "Regularization in **XGBoost** plays a crucial role in **controlling overfitting**, **improving generalization**, and\n",
        "**stabilizing model training**. Unlike traditional Gradient Boosting, XGBoost introduces **L1 (Lasso), L2 (Ridge), and\n",
        "Tree-based (Gamma) regularization**, making it more powerful and robust.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Types of Regularization in XGBoost**\n",
        "\n",
        "XGBoost introduces **three** types of regularization:\n",
        "\n",
        "### **âœ… L1 Regularization (Lasso) â€“ Shrinks Feature Weights**\n",
        "- Encourages **sparsity** by forcing some feature weights to **zero**, removing less important features.\n",
        "- Helps with **feature selection**.\n",
        "- Controlled by **`reg_alpha (Î±)`** in XGBoost.\n",
        "\n",
        "### **âœ… L2 Regularization (Ridge) â€“ Smooths Model Complexity**\n",
        "- Penalizes large weight values to prevent overfitting.\n",
        "- Helps distribute importance across features, improving generalization.\n",
        "- Controlled by **`reg_lambda (Î»)`** in XGBoost.\n",
        "\n",
        "### **âœ… Tree-Specific Regularization (Gamma - `Î³`)**\n",
        "- **Controls tree complexity** by adding a penalty for each additional leaf node.\n",
        "- Higher Î³ â†’ simpler trees (**reduces overfitting**).\n",
        "- Lower Î³ â†’ deeper trees (**captures more complex patterns**).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Regularization in XGBoostâ€™s Objective Function**\n",
        "\n",
        "XGBoost optimizes the following objective function:\n",
        "\n",
        "\\[\n",
        "\\text{Objective} = \\sum L(y, \\hat{y}) + \\lambda \\sum w^2 + \\alpha \\sum |w| + \\gamma T\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **\\( L(y, \\hat{y}) \\)** â†’ Loss function (e.g., Log Loss, RMSE).\n",
        "- **\\( \\lambda \\sum w^2 \\)** â†’ L2 regularization (controls large weights).\n",
        "- **\\( \\alpha \\sum |w| \\)** â†’ L1 regularization (encourages sparsity).\n",
        "- **\\( \\gamma T \\)** â†’ Tree complexity penalty (restricts tree depth & size).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Benefits of Regularization in XGBoost**\n",
        "\n",
        "âœ… **Prevents Overfitting** â€“ Controls model complexity and reduces high variance.\n",
        "âœ… **Feature Selection** â€“ L1 regularization removes unimportant features automatically.\n",
        "âœ… **Better Generalization** â€“ Model performs well on unseen data.\n",
        "âœ… **Handles Noisy Data** â€“ Avoids learning from irrelevant patterns.\n",
        "âœ… **Efficient Training** â€“ Reduces the risk of overcomplicated models.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Tuning Regularization Parameters in Python**\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost with regularization\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=100, learning_rate=0.1, max_depth=3,\n",
        "    reg_lambda=1,  # L2 regularization (default: 1)\n",
        "    reg_alpha=0.5, # L1 regularization (default: 0)\n",
        "    gamma=0.2,     # Tree complexity control (default: 0)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **5. When to Adjust Regularization?**\n",
        "ðŸ”¹ **Increase** `reg_lambda` (L2) if the model is overfitting and you want to **smooth out feature weights**.\n",
        "ðŸ”¹ **Increase** `reg_alpha` (L1) if you want to **remove irrelevant features** automatically.\n",
        "ðŸ”¹ **Increase** `gamma` if the model is **growing too deep**, leading to overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "nhJ2veAFy-be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11  What are some hyperparameters to tune in Gradient Boosting models\n",
        "\n",
        "#Answer.  ### **Hyperparameters to Tune in Gradient Boosting Models** ðŸš€\n",
        "\n",
        "Tuning hyperparameters in **Gradient Boosting Models (GBM, XGBoost, LightGBM, CatBoost)** is crucial\n",
        "for improving performance and preventing overfitting. Below are the key hyperparameters to tune, categorized by their function.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Tree Structure Hyperparameters ðŸŒ³**\n",
        "These control how deep and complex the decision trees grow.\n",
        "\n",
        "âœ… **`max_depth`** â†’ Maximum depth of each tree.\n",
        "   - Higher â†’ More complex model (risk of overfitting).\n",
        "   - Lower â†’ Simpler model (risk of underfitting).\n",
        "   - ðŸ”§ Typical range: `3-10`\n",
        "\n",
        "âœ… **`min_child_weight`** â†’ Minimum sum of instance weights needed to create a leaf node.\n",
        "   - Higher â†’ More conservative (reduces overfitting).\n",
        "   - Lower â†’ More flexible (allows deeper trees).\n",
        "   - ðŸ”§ Typical range: `1-10`\n",
        "\n",
        "âœ… **`gamma` (XGBoost) / `min_gain_to_split` (LightGBM)** â†’ Minimum loss reduction required to split a node.\n",
        "   - Higher â†’ Prevents unnecessary splits (reduces overfitting).\n",
        "   - ðŸ”§ Typical range: `0-5`\n",
        "\n",
        "âœ… **`colsample_bytree`** â†’ Fraction of features randomly selected per tree.\n",
        "   - Lower â†’ Prevents overfitting.\n",
        "   - ðŸ”§ Typical range: `0.5-1.0`\n",
        "\n",
        "âœ… **`colsample_bylevel`** (XGBoost) â†’ Fraction of features selected per tree level.\n",
        "\n",
        "âœ… **`colsample_bynode`** (XGBoost) â†’ Fraction of features selected per node split.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Boosting Process Hyperparameters ðŸš€**\n",
        "These define how boosting iterations occur.\n",
        "\n",
        "âœ… **`n_estimators`** â†’ Number of boosting iterations (trees).\n",
        "   - Higher â†’ More trees (risk of overfitting).\n",
        "   - ðŸ”§ Typical range: `100-1000` (use **early stopping**).\n",
        "\n",
        "âœ… **`learning_rate`** (also called `eta`) â†’ Shrinks contribution of each tree.\n",
        "   - Lower â†’ More robust but needs more trees.\n",
        "   - ðŸ”§ Typical range: `0.01-0.3`\n",
        "\n",
        "âœ… **`subsample`** â†’ Fraction of training data randomly sampled per boosting round.\n",
        "   - Lower â†’ Prevents overfitting.\n",
        "   - ðŸ”§ Typical range: `0.5-1.0`\n",
        "\n",
        "âœ… **`boosting_type`** (LightGBM, CatBoost)\n",
        "   - Options: `\"gbdt\"`, `\"dart\"`, `\"rf\"`.\n",
        "   - `\"dart\"` helps regularization by dropping trees randomly.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Regularization Hyperparameters ðŸ”¥**\n",
        "These control overfitting.\n",
        "\n",
        "âœ… **`reg_alpha` (L1 Regularization)** â†’ Shrinks some feature weights to zero.\n",
        "   - ðŸ”§ Typical range: `0-10`\n",
        "\n",
        "âœ… **`reg_lambda` (L2 Regularization)** â†’ Penalizes large weight values.\n",
        "   - ðŸ”§ Typical range: `0-10`\n",
        "\n",
        "âœ… **`gamma` (XGBoost) / `min_gain_to_split` (LightGBM)** â†’ Minimum reduction in loss required to make a split.\n",
        "   - ðŸ”§ Typical range: `0-5`\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Special Hyperparameters for Different Boosting Models**\n",
        "\n",
        "âœ… **XGBoost-Specific**\n",
        "- `tree_method`: `\"hist\"` (faster on large data), `\"gpu_hist\"` (for GPU).\n",
        "- `grow_policy`: `\"depthwise\"` (standard), `\"lossguide\"` (adaptive depth).\n",
        "\n",
        "âœ… **LightGBM-Specific**\n",
        "- `num_leaves`: Controls tree complexity (`2^(max_depth)`).\n",
        "- `max_bin`: Number of bins for feature values (higher = better splits but slower).\n",
        "\n",
        "âœ… **CatBoost-Specific**\n",
        "- `depth`: Tree depth (similar to `max_depth` but optimized for categorical features).\n",
        "- `one_hot_max_size`: Maximum categorical features to one-hot encode (default = 2).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Best Practices for Hyperparameter Tuning ðŸŽ¯**\n",
        "\n",
        "ðŸ”¹ **Use Grid Search / Random Search / Bayesian Optimization**\n",
        "- `GridSearchCV`: Systematic tuning (slow but thorough).\n",
        "- `RandomizedSearchCV`: Random sampling (faster).\n",
        "- `Optuna` or `Hyperopt`: Bayesian optimization (smart search).\n",
        "\n",
        "ðŸ”¹ **Use Early Stopping**\n",
        "- Stops training when validation performance stops improving.\n",
        "\n",
        "ðŸ”¹ **Balance Between `learning_rate` and `n_estimators`**\n",
        "- If `learning_rate` is low, increase `n_estimators`.\n",
        "- If `n_estimators` is too high, reduce `learning_rate`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example: Hyperparameter Tuning in XGBoost**\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define model\n",
        "xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'reg_lambda': [1, 10, 100]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts ðŸ’¡**\n",
        "ðŸ”¹ **Start with default hyperparameters** and tune step by step.\n",
        "ðŸ”¹ **Regularization parameters (`reg_alpha`, `reg_lambda`)** help prevent overfitting.\n",
        "ðŸ”¹ **Use feature selection and early stopping** for faster tuning."
      ],
      "metadata": {
        "id": "KJ-EqiTLy-Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 What is the concept of Feature Importance in Boosting\n",
        "\n",
        "#Answer. ## **Feature Importance in Boosting** ðŸš€\n",
        "\n",
        "**Feature Importance** in Boosting models (like XGBoost, LightGBM, and CatBoost) refers to how much each feature\n",
        "contributes to the modelâ€™s predictions. It helps in **feature selection**, **interpretability**, and **reducing overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Types of Feature Importance in Boosting Models**\n",
        "\n",
        "Boosting models provide different ways to measure feature importance:\n",
        "\n",
        "### **âœ… 1. Gain-Based Importance (Most Common)**\n",
        "- Measures how much a feature **reduces the loss function** when used in a split.\n",
        "- Higher gain â†’ More important feature.\n",
        "- **Used in XGBoost, LightGBM, CatBoost.**\n",
        "\n",
        "### **âœ… 2. Split Count-Based Importance**\n",
        "- Measures how **many times** a feature is used in tree splits.\n",
        "- More splits â†’ Feature is frequently used (but not necessarily better).\n",
        "- **Used in XGBoost & LightGBM.**\n",
        "\n",
        "### **âœ… 3. Permutation Importance (Post-training)**\n",
        "- Measures feature importance by **shuffling** values and checking performance drop.\n",
        "- More accurate but slower.\n",
        "- **Works with any model.**\n",
        "\n",
        "### **âœ… 4. SHAP (Shapley Additive Explanations)**\n",
        "- Advanced method that **considers feature interactions**.\n",
        "- More interpretable than gain-based methods.\n",
        "- **Used in Explainable AI (XAI).**\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Visualizing Feature Importance in Python**\n",
        "\n",
        "### **ðŸ“Œ Using XGBoost**\n",
        "```python\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_importance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plot_importance(model, importance_type=\"gain\")  # \"gain\", \"weight\", or \"cover\"\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. When to Use Feature Importance?**\n",
        "\n",
        "âœ… **Feature Selection** â€“ Remove less important features to improve performance.\n",
        "âœ… **Interpretability** â€“ Understand which features drive predictions.\n",
        "âœ… **Reduce Overfitting** â€“ Drop irrelevant or noisy features.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. SHAP for Deeper Feature Importance Analysis**\n",
        "```python\n",
        "import shap\n",
        "\n",
        "# Explain model predictions\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Takeaways ðŸ’¡**\n",
        "ðŸ”¹ **Gain-Based Importance** is most common but can be biased.\n",
        "ðŸ”¹ **SHAP Importance** is better for interpretability.\n",
        "ðŸ”¹ **Use Feature Selection** to remove unimportant variables and boost model performance.\n"
      ],
      "metadata": {
        "id": "dPIkt_0jy-Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Why is CatBoost efficient for categorical data?\n",
        "\n",
        "#Answer. ## **Why is CatBoost Efficient for Categorical Data?** ðŸš€\n",
        "\n",
        "**CatBoost (Categorical Boosting)** is specifically designed to handle categorical features efficiently, making it **faster,\n",
        "more accurate, and easier to use** compared to other boosting algorithms like XGBoost and LightGBM. Hereâ€™s why:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Native Handling of Categorical Features (No One-Hot Encoding Required)**\n",
        "Unlike XGBoost or LightGBM, **CatBoost does not require one-hot encoding or label encoding**. Instead, it processes\n",
        "categorical variables directly using an advanced encoding technique called **Ordered Target Statistics (Ordered Boosting)**.\n",
        "\n",
        "ðŸ”¹ **Why is this better?**\n",
        "âœ… Avoids **high-dimensional feature explosion** (from one-hot encoding).\n",
        "âœ… Prevents **information leakage** (common in traditional encoding).\n",
        "âœ… Works well with **high-cardinality features** (features with many unique values).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Ordered Target Encoding (Avoids Data Leakage)**\n",
        "CatBoost uses a **unique Ordered Target Encoding** for categorical variables.\n",
        "\n",
        "ðŸ”¹ **How does it work?**\n",
        "- Instead of using the mean target value of a category (which causes data leakage), CatBoost **splits the dataset into\n",
        " multiple random permutations** and computes target statistics **only on past data points**.\n",
        "- This prevents the model from using **future information** while training.\n",
        "\n",
        "ðŸ”¹ **Why is this better?**\n",
        "âœ… **Prevents overfitting** by avoiding leakage.\n",
        "âœ… **More accurate predictions** with categorical features.\n",
        "âœ… **Works well on small datasets** with many categories.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Built-in Handling of Missing Categorical Values**\n",
        "- CatBoost **automatically processes missing categorical values** without requiring manual imputation.\n",
        "- It treats missing values as a separate category, reducing data preprocessing effort.\n",
        "\n",
        "ðŸ”¹ **Why is this better?**\n",
        "âœ… No need for **imputation** (which may introduce bias).\n",
        "âœ… Handles **incomplete categorical data** gracefully.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Efficient GPU Implementation for Categorical Features**\n",
        "- CatBoost efficiently handles categorical features even when training on **GPUs**, unlike other boosting algorithms\n",
        "that struggle with categorical data on GPU.\n",
        "\n",
        "ðŸ”¹ **Why is this better?**\n",
        "âœ… **Fast training speed** (even on large datasets).\n",
        "âœ… **Optimized for categorical variables** (without heavy preprocessing).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. How to Use CatBoost with Categorical Data?**\n",
        "Hereâ€™s a simple example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset with categorical features\n",
        "data = pd.DataFrame({\n",
        "    'color': ['red', 'blue', 'green', 'red', 'blue', 'green', 'red'],\n",
        "    'size': ['S', 'M', 'L', 'M', 'S', 'L', 'M'],\n",
        "    'price': [10, 15, 20, 25, 30, 35, 40],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "# Convert categorical columns to type 'category'\n",
        "cat_features = ['color', 'size']\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Model\n",
        "model = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, cat_features=cat_features, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **6. When to Use CatBoost?**\n",
        "ðŸ”¹ When you have **many categorical features** (e.g., customer data, product categories).\n",
        "ðŸ”¹ When categorical features have **many unique values** (e.g., ZIP codes, user IDs).\n",
        "ðŸ”¹ When you want to **avoid complex preprocessing** (like one-hot encoding).\n",
        "ðŸ”¹ When using **small datasets**, since CatBoostâ€™s **ordered encoding prevents overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Takeaways ðŸ’¡**\n",
        "âœ… **No One-Hot Encoding Needed** â†’ Saves memory and improves speed.\n",
        "âœ… **Ordered Target Encoding** â†’ Prevents data leakage and overfitting.\n",
        "âœ… **Handles Missing Categorical Data** â†’ No manual imputation required.\n",
        "âœ… **Optimized for GPUs** â†’ Faster training with categorical variables.\n"
      ],
      "metadata": {
        "id": "KylQi1uBy-Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                  # PRACTICAL"
      ],
      "metadata": {
        "id": "-QJnbxGSy-Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "\n",
        "#Ans Hereâ€™s how you can train an **AdaBoost Classifier** on a sample dataset and print the model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Dataset**\n",
        "We generate a synthetic classification dataset with **1000 samples and 10 features**.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the AdaBoost Model**\n",
        "We use a **Decision Tree (stump)** as the base estimator.\n",
        "```python\n",
        "# Define AdaBoost classifier with a Decision Tree base model\n",
        "adaboost = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (stump)\n",
        "    n_estimators=50,  # Number of boosting rounds\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "adaboost.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Accuracy**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output**\n",
        "```\n",
        "Model Accuracy: 0.85  (varies based on dataset)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use AdaBoost?**\n",
        "âœ… **Boosts weak learners** (e.g., decision stumps) into a strong classifier.\n",
        "âœ… **Focuses on misclassified points** and adjusts weights accordingly.\n",
        "âœ… **Performs well with small datasets** and avoids overfitting.\n"
      ],
      "metadata": {
        "id": "0SLfh5KBy-Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "\n",
        "#Answer. Hereâ€™s how you can train an **AdaBoost Regressor** and evaluate its performance using **Mean Absolute Error (MAE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Regression Dataset**\n",
        "We generate a synthetic regression dataset with **1000 samples and 10 features**.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the AdaBoost Regressor**\n",
        "We use a **Decision Tree (stump)** as the base estimator.\n",
        "```python\n",
        "# Define AdaBoost Regressor with a Decision Tree base model\n",
        "adaboost_reg = AdaBoostRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(max_depth=3),  # Weak learner\n",
        "    n_estimators=50,  # Number of boosting rounds\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance using MAE**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "\n",
        "# Compute Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Mean Absolute Error (MAE): 5.1234\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use AdaBoost for Regression?**\n",
        "âœ… **Boosts weak regressors** into a stronger model.\n",
        "âœ… **Focuses on hard-to-predict data points** by adjusting sample weights.\n",
        "âœ… **Works well on small-to-medium-sized datasets**.\n"
      ],
      "metadata": {
        "id": "aaJzRCF3y-Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16  Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "\n",
        "\n",
        "#Answer. Hereâ€™s how you can train a **Gradient Boosting Classifier** on the **Breast Cancer dataset** and print the **feature importance**. ðŸš€\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the Gradient Boosting Classifier**\n",
        "```python\n",
        "# Define Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_clf.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Accuracy**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 5: Print and Plot Feature Importance**\n",
        "```python\n",
        "# Get feature importance\n",
        "feature_importance = gb_clf.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"\\nFeature Importance:\\n\", importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in Gradient Boosting Classifier\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output**\n",
        "```\n",
        "Model Accuracy: 0.9649\n",
        "Feature Importance:\n",
        "                     Feature  Importance\n",
        "1        mean texture     0.19\n",
        "5     mean compactness     0.12\n",
        "10     worst texture     0.09\n",
        "...   ...  ...\n",
        "```\n",
        "(Values will vary slightly based on training)\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use Feature Importance?**\n",
        "âœ… Helps in **feature selection** â€“ Remove less important features.\n",
        "âœ… Improves **model interpretability** â€“ Understand which features contribute most.\n",
        "âœ… Can **reduce overfitting** by eliminating noisy features.\n"
      ],
      "metadata": {
        "id": "p-bxYKcdy-Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "\n",
        "#Answer. Hereâ€™s how you can train a **Gradient Boosting Regressor** and evaluate it using the **R-Squared (RÂ²) Score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import r2_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Regression Dataset**\n",
        "We generate a synthetic regression dataset with **1000 samples and 10 features**.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the Gradient Boosting Regressor**\n",
        "```python\n",
        "# Define Gradient Boosting model\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance using RÂ² Score**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "\n",
        "# Compute R-Squared Score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-Squared Score: {r2:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "R-Squared Score: 0.92  (Higher is better, ideally close to 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use R-Squared Score?**\n",
        "âœ… Measures how well the model explains variance in the data.\n",
        "âœ… RÂ² = 1 means **perfect prediction**, RÂ² = 0 means **no predictive power**.\n",
        "âœ… Helps assess **model quality and performance**."
      ],
      "metadata": {
        "id": "oYpaXn2Zy-Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "\n",
        "#Answer.Hereâ€™s how you can train both an **XGBoost Classifier** and a **Gradient Boosting Classifier**, compare their accuracy,\n",
        "and determine which model performs better\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "Make sure you have **XGBoost** installed. If not, install it using:\n",
        "```bash\n",
        "pip install xgboost\n",
        "```\n",
        "Now, import required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, a commonly used dataset for binary classification tasks.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the Gradient Boosting Classifier**\n",
        "```python\n",
        "# Define Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Train the XGBoost Classifier**\n",
        "```python\n",
        "# Define XGBoost model\n",
        "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 5: Compare Model Performance**\n",
        "```python\n",
        "# Print accuracy comparison\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# Plot comparison\n",
        "plt.bar([\"Gradient Boosting\", \"XGBoost\"], [gb_accuracy, xgb_accuracy], color=['blue', 'red'])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Gradient Boosting vs XGBoost Accuracy\")\n",
        "plt.ylim(0.9, 1)  # Scale y-axis for better comparison\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Slightly)**\n",
        "```\n",
        "Gradient Boosting Accuracy: 0.9561\n",
        "XGBoost Accuracy: 0.9737\n",
        "```\n",
        "The plot will visually compare the two models.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Key Observations**\n",
        "âœ… **XGBoost is often more accurate** due to its advanced optimizations.\n",
        "âœ… **XGBoost is faster** because of parallel processing and regularization.\n",
        "âœ… **Gradient Boosting can perform well** but might need more tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "W9jXwBZmy-A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19  Train a CatBoost Classifier and evaluate using F1-Score4\n",
        "\n",
        "\n",
        "#Answer. Hereâ€™s how you can train a **CatBoost Classifier** and evaluate it using the **F1-Score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **CatBoost**, do so using:\n",
        "```bash\n",
        "pip install catboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification tasks.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the CatBoost Classifier**\n",
        "```python\n",
        "# Define CatBoost model\n",
        "catboost_clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "catboost_clf.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance using F1-Score**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = catboost_clf.predict(X_test)\n",
        "\n",
        "# Compute F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "F1-Score: 0.9725  (Higher is better, ideally close to 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use CatBoost?**\n",
        "âœ… Handles **categorical features efficiently** without encoding.\n",
        "âœ… **Faster training** than other boosting models.\n",
        "âœ… Works well **with small datasets** and **avoids overfitting**.\n"
      ],
      "metadata": {
        "id": "8KAA0Esmy9-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 4 Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "#Answer. Hereâ€™s how you can train an **XGBoost Regressor** and evaluate it using **Mean Squared Error (MSE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **XGBoost**, do so using:\n",
        "```bash\n",
        "pip install xgboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Regression Dataset**\n",
        "We generate a synthetic regression dataset with **1000 samples and 10 features**.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the XGBoost Regressor**\n",
        "```python\n",
        "# Define XGBoost Regressor\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance using MSE**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "# Compute Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Mean Squared Error (MSE): 125.67  (Lower is better)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use XGBoost for Regression?**\n",
        "âœ… **Handles large datasets efficiently**\n",
        "âœ… **Optimized for speed & performance**\n",
        "âœ… **Regularization prevents overfitting**\n",
        "\n"
      ],
      "metadata": {
        "id": "EHjz2Pymy97Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21  Train an AdaBoost Classifier and visualize feature importance\n",
        "\n",
        "#Answer. Hereâ€™s how you can train an **AdaBoost Classifier** and visualize the **feature importance**\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification tasks.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the AdaBoost Classifier**\n",
        "```python\n",
        "# Define AdaBoost Classifier with a Decision Tree base model\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (stump)\n",
        "    n_estimators=50,  # Number of boosting rounds\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 5: Visualize Feature Importance**\n",
        "```python\n",
        "# Get feature importance\n",
        "feature_importance = adaboost_clf.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"\\nFeature Importance:\\n\", importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in AdaBoost Classifier\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Model Accuracy: 0.9474\n",
        "Feature Importance:\n",
        "              Feature  Importance\n",
        "5  mean compactness        0.23\n",
        "1     mean texture        0.18\n",
        "10   worst texture        0.12\n",
        "...\n",
        "```\n",
        "A bar chart will also display feature importance.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use AdaBoost for Classification?**\n",
        "âœ… **Boosts weak classifiers** into a strong model.\n",
        "âœ… **Focuses on hard-to-classify instances** by adjusting sample weights.\n",
        "âœ… **Works well on imbalanced datasets**.\n",
        "\n"
      ],
      "metadata": {
        "id": "XdE5eJiCy94g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 4 Train a Gradient Boosting Regressor and plot learning curves\n",
        "\n",
        "\n",
        "#Answer.  Hereâ€™s how you can **train a Gradient Boosting Regressor** and **plot learning curves** to analyze training performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Regression Dataset**\n",
        "We generate a **synthetic regression dataset** with 1000 samples and 10 features.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the Gradient Boosting Regressor**\n",
        "```python\n",
        "# Define Gradient Boosting model\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Model Performance using MSE**\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "\n",
        "# Compute Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 5: Plot Learning Curves**\n",
        "Learning curves help analyze **bias-variance tradeoff** and detect overfitting.\n",
        "```python\n",
        "# Compute learning curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    gb_reg, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\", train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Convert negative MSE to positive\n",
        "train_scores_mean = -train_scores.mean(axis=1)\n",
        "test_scores_mean = -test_scores.mean(axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label=\"Training Error\", marker=\"o\", color=\"blue\")\n",
        "plt.plot(train_sizes, test_scores_mean, label=\"Validation Error\", marker=\"s\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Gradient Boosting Regressor Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Mean Squared Error (MSE): 125.67  (Lower is better)\n",
        "```\n",
        "The **learning curve plot** will show **training vs validation error**, helping analyze:\n",
        "âœ… **Underfitting** (both errors high)\n",
        "âœ… **Overfitting** (training error low, validation error high)\n",
        "âœ… **Optimal learning** (errors close together, both low)"
      ],
      "metadata": {
        "id": "-Py8Ss5Ey91b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Train an XGBoost Classifier and visualize feature importance\n",
        "\n",
        "\n",
        "#Answer.Hereâ€™s how you can **train an XGBoost Classifier** and **visualize feature importance** using the Breast Cancer dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **XGBoost**, install it using:\n",
        "```bash\n",
        "pip install xgboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the XGBoost Classifier**\n",
        "```python\n",
        "# Define XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Visualize Feature Importance**\n",
        "```python\n",
        "# Get feature importance scores\n",
        "feature_importance = xgb_clf.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"\\nFeature Importance:\\n\", importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in XGBoost Classifier\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Model Accuracy: 0.9737\n",
        "Feature Importance:\n",
        "              Feature  Importance\n",
        "10   worst texture        0.21\n",
        "5  mean compactness        0.18\n",
        "1     mean texture        0.15\n",
        "...\n",
        "```\n",
        "A **bar chart** will display feature importance.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use XGBoost for Classification?**\n",
        "âœ… **Highly optimized and efficient**\n",
        "âœ… **Handles large datasets well**\n",
        "âœ… **Feature importance helps understand the model**"
      ],
      "metadata": {
        "id": "zC4LUztty9yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Train a CatBoost Classifier and plot the confusion matrix\n",
        "\n",
        "#Ans Hereâ€™s how you can **train a CatBoost Classifier** and **plot the confusion matrix** using the Breast Cancer dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **CatBoost**, install it using:\n",
        "```bash\n",
        "pip install catboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification tasks.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the CatBoost Classifier**\n",
        "```python\n",
        "# Define CatBoost model\n",
        "catboost_clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "catboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = catboost_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Plot the Confusion Matrix**\n",
        "```python\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Model Accuracy: 0.9725  (Higher is better)\n",
        "```\n",
        "A **heatmap confusion matrix** will be displayed showing **True Positives, False Positives, True Negatives, and False Negatives**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use CatBoost?**\n",
        "âœ… **Handles categorical data efficiently**\n",
        "âœ… **Faster training compared to other boosting models**\n",
        "âœ… **Works well with small datasets**\n"
      ],
      "metadata": {
        "id": "vsRVt6PCy9v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "\n",
        "\n",
        "#Answer.  Hereâ€™s how you can **train an AdaBoost Classifier** with different numbers of estimators and compare accuracy .\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train AdaBoost Classifier with Different Numbers of Estimators**\n",
        "We'll train the AdaBoost classifier with **varying numbers of estimators** and record accuracy.\n",
        "```python\n",
        "# Define different numbers of estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "\n",
        "# Store accuracy results\n",
        "accuracy_scores = []\n",
        "\n",
        "# Loop through different n_estimators values\n",
        "for n in n_estimators_list:\n",
        "    # Define AdaBoost model\n",
        "    adaboost_clf = AdaBoostClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (stump)\n",
        "        n_estimators=n,\n",
        "        learning_rate=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Estimators: {n}, Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Plot Accuracy vs. Number of Estimators**\n",
        "```python\n",
        "# Plot the accuracy trend\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, accuracy_scores, marker='o', linestyle='-', color='blue', label=\"Accuracy\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier: Accuracy vs. Number of Estimators\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Estimators: 10, Accuracy: 0.9298\n",
        "Estimators: 50, Accuracy: 0.9474\n",
        "Estimators: 100, Accuracy: 0.9649\n",
        "Estimators: 200, Accuracy: 0.9737\n",
        "Estimators: 500, Accuracy: 0.9737\n",
        "```\n",
        "A **line plot** will show the accuracy trend as the number of estimators increases.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Key Observations**\n",
        "âœ… Increasing **n_estimators** improves accuracy up to a point.\n",
        "âœ… After a certain number, accuracy **plateaus** (no further improvement).\n",
        "âœ… Using **too many estimators** may lead to **overfitting**.\n"
      ],
      "metadata": {
        "id": "GEKiTglQy9s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "\n",
        "#Answer. Hereâ€™s how you can **train a Gradient Boosting Classifier** and **visualize the ROC curve** using the Breast Cancer dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Breast Cancer Dataset**\n",
        "Weâ€™ll use the **Breast Cancer dataset**, which is great for binary classification tasks.\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Feature matrix\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the Gradient Boosting Classifier**\n",
        "```python\n",
        "# Define Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "y_proba = gb_clf.predict_proba(X_test)[:, 1]  # Probability scores for the positive class\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Plot the ROC Curve**\n",
        "```python\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier line\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"Gradient Boosting Classifier - ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Model Accuracy: 0.9649  (Higher is better)\n",
        "```\n",
        "A **ROC curve plot** will be displayed, showing the trade-off between **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why is the ROC Curve Useful?**\n",
        "âœ… Helps **evaluate classifier performance** across different thresholds.\n",
        "âœ… **Higher AUC (Area Under Curve) = Better Model Performance**.\n",
        "âœ… Useful for **imbalanced datasets** where accuracy alone is misleading.\n",
        "\n"
      ],
      "metadata": {
        "id": "yayyJQeny9qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27  Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "\n",
        "#Answer. Hereâ€™s how you can **train an XGBoost Regressor** and **tune the learning rate using GridSearchCV**. ðŸš€\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **XGBoost**, install it using:\n",
        "```bash\n",
        "pip install xgboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create a Sample Regression Dataset**\n",
        "We generate a **synthetic regression dataset** with 1000 samples and 10 features.\n",
        "```python\n",
        "# Generate dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Define XGBoost Regressor & Tune Learning Rate using GridSearchCV**\n",
        "We will search for the **best learning rate** using **GridSearchCV**.\n",
        "```python\n",
        "# Define XGBoost Regressor\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]  # Different learning rates to test\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best learning rate\n",
        "grid_search = GridSearchCV(xgb_reg, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best learning rate\n",
        "best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Train XGBoost with the Best Learning Rate & Evaluate Performance**\n",
        "```python\n",
        "# Train the best model\n",
        "best_xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=best_learning_rate, max_depth=3, random_state=42)\n",
        "best_xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_xgb_reg.predict(X_test)\n",
        "\n",
        "# Compute Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Fitting 5 folds for each of 5 candidates, totaling 25 fits\n",
        "Best Learning Rate: 0.1\n",
        "Mean Squared Error (MSE): 125.67  (Lower is better)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Tune the Learning Rate?**\n",
        "âœ… **Too high:** Model **overfits** (memorizes data but fails on new data).\n",
        "âœ… **Too low:** Model **underfits** (learns too slowly, poor predictions).\n",
        "âœ… **Optimal learning rate** balances accuracy and generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "jRrzguw4y9nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "\n",
        "\n",
        "#Answer. ### **Train a CatBoost Classifier on an Imbalanced Dataset and Compare Performance with Class Weighting**\n",
        "\n",
        "Handling **imbalanced datasets** is crucial in classification problems to avoid biased models. Here, we train **CatBoost** on an\n",
        "imbalanced dataset and compare its performance **with and without class weighting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **CatBoost**, install it using:\n",
        "```bash\n",
        "pip install catboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Create an Imbalanced Dataset**\n",
        "We generate a **highly imbalanced dataset** where **class 0 is much more frequent than class 1**.\n",
        "```python\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Check class distribution\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"Class Distribution: {dict(zip(unique, counts))}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train CatBoost Without Class Weighting**\n",
        "```python\n",
        "# Train CatBoost without class weighting\n",
        "catboost_clf = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = catboost_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Performance WITHOUT Class Weights:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Without Class Weighting\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Train CatBoost WITH Class Weighting**\n",
        "Class weights help the model **focus on the minority class**. We calculate weights as **(Total Samples / Class Count)**.\n",
        "```python\n",
        "# Calculate class weights\n",
        "class_weights = {0: len(y_train) / (2 * np.bincount(y_train)[0]), 1: len(y_train) / (2 * np.bincount(y_train)[1])}\n",
        "\n",
        "# Train CatBoost with class weighting\n",
        "catboost_clf_weighted = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, class_weights=class_weights, verbose=0, random_state=42)\n",
        "catboost_clf_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_weighted = catboost_clf_weighted.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Performance WITH Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
        "sns.heatmap(conf_matrix_weighted, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - With Class Weighting\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "Without class weighting:\n",
        "- **High accuracy** but poor recall for minority class.\n",
        "- Model predicts mostly majority class (Class 0).\n",
        "\n",
        "With class weighting:\n",
        "- **Improved recall for Class 1** (better minority class detection).\n",
        "- **Balanced precision-recall tradeoff**.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use Class Weights?**\n",
        "âœ… Helps **reduce bias** toward the majority class.\n",
        "âœ… Increases **recall** for the minority class.\n",
        "âœ… More **balanced** precision-recall tradeoff.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWtfAG19y9k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "\n",
        "\n",
        "#Answer  ### **Train an AdaBoost Classifier and Analyze the Effect of Different Learning Rates**\n",
        "\n",
        "Learning rate **controls the contribution** of each weak learner in **AdaBoost**.\n",
        "- **High learning rate** â†’ Faster learning but risk of overfitting.\n",
        "- **Low learning rate** â†’ Slower learning but better generalization.\n",
        "Weâ€™ll **train AdaBoost with different learning rates** and analyze accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Import Necessary Libraries**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load the Dataset**\n",
        "We use the **Breast Cancer dataset** (binary classification).\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train AdaBoost with Different Learning Rates**\n",
        "```python\n",
        "# Define learning rates to test\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "# Store accuracy results\n",
        "accuracy_scores = []\n",
        "\n",
        "# Loop through learning rates\n",
        "for lr in learning_rates:\n",
        "    # Define AdaBoost model\n",
        "    adaboost_clf = AdaBoostClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=100,\n",
        "        learning_rate=lr,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Learning Rate: {lr}, Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Plot Accuracy vs. Learning Rate**\n",
        "```python\n",
        "# Plot accuracy trend\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, accuracy_scores, marker='o', linestyle='-', color='blue', label=\"Accuracy\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.title(\"AdaBoost Classifier: Accuracy vs. Learning Rate\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Learning Rate: 0.001, Accuracy: 0.8947\n",
        "Learning Rate: 0.01, Accuracy: 0.9386\n",
        "Learning Rate: 0.1, Accuracy: 0.9649\n",
        "Learning Rate: 0.5, Accuracy: 0.9737\n",
        "Learning Rate: 1.0, Accuracy: 0.9474\n",
        "Learning Rate: 2.0, Accuracy: 0.9123\n",
        "```\n",
        "A **line plot** will show accuracy vs. learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Key Observations**\n",
        "âœ… **Too small (0.001, 0.01)** â†’ Model **learns too slowly** (low accuracy).\n",
        "âœ… **Optimal (0.1 - 0.5)** â†’ Best balance between learning speed & accuracy.\n",
        "âœ… **Too high (2.0)** â†’ Model **overfits** or diverges (accuracy drops).\n"
      ],
      "metadata": {
        "id": "7e6dzdecy9h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #30 Train an XGBoost Classifier for multi-class classification and evaluate using log-loss\n",
        "\n",
        " #answer ### **Train an XGBoost Classifier for Multi-Class Classification and Evaluate Using Log-Loss**\n",
        "\n",
        "Log-loss (**logarithmic loss**) measures how well a classifier predicts **probability scores** for multiple classes. Lower values indicate better performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 1: Install & Import Necessary Libraries**\n",
        "If you havenâ€™t installed **XGBoost**, install it using:\n",
        "```bash\n",
        "pip install xgboost\n",
        "```\n",
        "Now, import the required libraries:\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 2: Load Multi-Class Dataset**\n",
        "We use the **Digits dataset** (handwritten digit classification, 10 classes).\n",
        "```python\n",
        "# Load dataset\n",
        "data = load_digits()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target  # Multi-class labels (digits 0-9)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 3: Train the XGBoost Multi-Class Classifier**\n",
        "```python\n",
        "# Define XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective=\"multi:softprob\",  # Multi-class classification\n",
        "    num_class=10,  # 10 classes (digits 0-9)\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = xgb_clf.predict_proba(X_test)  # Predict probabilities\n",
        "y_pred = xgb_clf.predict(X_test)  # Predict class labels\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Step 4: Evaluate Performance Using Log-Loss & Accuracy**\n",
        "```python\n",
        "# Compute Log-Loss\n",
        "logloss = log_loss(y_test, y_pred_prob)\n",
        "print(f\"Log-Loss: {logloss:.4f}\")\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“Œ Expected Output (Varies Based on Data)**\n",
        "```\n",
        "Log-Loss: 0.3214  (Lower is better)\n",
        "Accuracy: 0.9556  (Higher is better)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ”¹ Why Use Log-Loss for Multi-Class?**\n",
        "âœ… **Evaluates probability confidence** (not just class labels).\n",
        "âœ… **Punishes incorrect confident predictions more** (e.g., assigning 90% to the wrong class).\n",
        "âœ… Useful for models used in **ranking or probabilistic decision-making**.\n"
      ],
      "metadata": {
        "id": "a5xrObedy9eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3rMwXnKy9cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQCrh3_Cy9ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIMUM6puy9Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFDL567jy9T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLIuJmTLy9Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-08m1NMy9Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7oArRedy9MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaK5y__ky9JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qaui2SQy9GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B9L31oYKy9Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sht4TeaWy9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyZ3AIOIy8_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0VrPrrQy88e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGthbz8ky85h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54Idq5dsy824"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}