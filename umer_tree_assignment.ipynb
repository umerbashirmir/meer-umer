{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4A_a2pFES54"
      },
      "outputs": [],
      "source": [
        "#1. What is a Decision Tree, and how does it work\n",
        "\n",
        "#Ans. A **Decision Tree** is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "It is a tree-like model of decisions, where each node represents a feature (attribute), each branch represents a\n",
        "decision rule, and each leaf node represents an outcome (class label or numerical value).\n",
        "\n",
        "### **How It Works:**\n",
        "1. **Splitting:**\n",
        "   - The dataset is split into subsets based on feature values.\n",
        "   - The algorithm selects the best feature to split the data using criteria like **Gini Impurity**,\n",
        "   **Entropy (Information Gain)**, or **Mean Squared Error (MSE)** (for regression).\n",
        "\n",
        "2. **Decision Nodes and Branches:**\n",
        "   - At each node, the data is further split based on specific conditions.\n",
        "   - This process continues recursively, creating branches.\n",
        "\n",
        "3. **Leaf Nodes:**\n",
        "   - The process stops when a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
        "   - Each leaf node represents a class label (classification) or a predicted value (regression).\n",
        "\n",
        "4. **Pruning (Optional):**\n",
        "   - To avoid overfitting, unnecessary branches are removed using **pre-pruning** (limiting depth, minimum samples per split)\n",
        "   or **post-pruning** (removing branches after training).\n",
        "\n",
        "### **Example of a Decision Tree:**\n",
        "Imagine a decision tree that predicts if someone will buy a laptop based on their income and age:\n",
        "\n",
        "```\n",
        "           Income?\n",
        "          /      \\\n",
        "     High        Low\n",
        "      |           |\n",
        "   Age < 30?     No\n",
        "   /       \\\n",
        " Yes       No\n",
        "  |         |\n",
        "Buy       Don't Buy\n",
        "```\n",
        "\n",
        "### **Advantages of Decision Trees:**\n",
        "‚úîÔ∏è Easy to understand and interpret\n",
        "‚úîÔ∏è Requires little data preprocessing\n",
        "‚úîÔ∏è Can handle both numerical and categorical data\n",
        "\n",
        "### **Disadvantages:**\n",
        "‚ùå Prone to overfitting if not pruned properly\n",
        "‚ùå Can be biased if data is imbalanced\n",
        "‚ùå Sensitive to small changes in data (leading to different splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 What are impurity measures in Decision Trees\n",
        "\n",
        "#Ans  ### **Impurity Measures in Decision Trees**\n",
        "Impurity measures determine how \"mixed\" the data is at a given node in a Decision Tree.\n",
        " The goal is to minimize impurity when splitting nodes so that each branch becomes more homogeneous.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Gini Impurity (Used in CART Algorithm)**\n",
        "**Formula:**\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "\\]\n",
        "Where:\n",
        "- \\( p_i \\) is the probability of class \\( i \\) in the node\n",
        "- \\( c \\) is the total number of classes\n",
        "\n",
        "**Interpretation:**\n",
        "- Gini = 0 ‚Üí Pure node (all instances belong to one class)\n",
        "- Gini = 0.5 ‚Üí Maximum impurity (equal distribution of two classes)\n",
        "\n",
        "üîπ **Example:** If a node contains 80% class A and 20% class B:\n",
        "\\[\n",
        "Gini = 1 - (0.8^2 + 0.2^2) = 0.32\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Entropy (Used in ID3 & C4.5 Algorithm)**\n",
        "**Formula:**\n",
        "\\[\n",
        "Entropy = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
        "\\]\n",
        "\n",
        "**Interpretation:**\n",
        "- Entropy = 0 ‚Üí Pure node\n",
        "- Entropy = 1 ‚Üí Maximum impurity (equal class distribution)\n",
        "\n",
        "üîπ **Example:** If a node contains 80% class A and 20% class B:\n",
        "\\[\n",
        "Entropy = - (0.8 \\log_2 0.8 + 0.2 \\log_2 0.2) \\approx 0.72\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Classification Error (Least Used)**\n",
        "**Formula:**\n",
        "\\[\n",
        "Error = 1 - \\max(p_i)\n",
        "\\]\n",
        "\n",
        "**Interpretation:**\n",
        "- Measures misclassification rate\n",
        "- Not as sensitive to small changes as Gini or Entropy\n",
        "\n",
        "üîπ **Example:** If a node has 80% class A and 20% class B:\n",
        "\\[\n",
        "Error = 1 - 0.8 = 0.2\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Mean Squared Error (MSE) for Regression Trees**\n",
        "**Formula:**\n",
        "\\[\n",
        "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n",
        "\\]\n",
        "\n",
        "- Measures the variance of target values at a node\n",
        "- The goal is to minimize MSE when splitting\n",
        "\n",
        "---\n",
        "\n",
        "### **Which One to Use?**\n",
        "- **Gini Impurity** is faster and preferred in **CART (Classification and Regression Trees)**.\n",
        "- **Entropy** gives better splits but is computationally heavier.\n",
        "- **MSE** is used for **regression trees**.\n",
        "\n"
      ],
      "metadata": {
        "id": "wAe06ldfEUMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3  What is the mathematical formula for Gini Impurity\n",
        "\n",
        "#Ans  ### **Mathematical Formula for Gini Impurity**\n",
        "\n",
        "The **Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element\n",
        "if it were randomly labeled according to the class distribution in a node.\n",
        "\n",
        "#### **Formula:**\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( c \\) = Total number of classes\n",
        "- \\( p_i \\) = Proportion (probability) of class \\( i \\) in the node\n",
        "\n",
        "#### **Alternative Representation:**\n",
        "\\[\n",
        "Gini = \\sum_{i=1}^{c} p_i (1 - p_i)\n",
        "\\]\n",
        "(since \\( 1 - p_i^2 = p_i(1 - p_i) + (1 - p_i)p_i \\))\n",
        "\n",
        "#### **Example Calculation:**\n",
        "Consider a dataset where a node contains two classes:\n",
        "- Class A: 80% (\\( p_A = 0.8 \\))\n",
        "- Class B: 20% (\\( p_B = 0.2 \\))\n",
        "\n",
        "\\[\n",
        "Gini = 1 - (0.8^2 + 0.2^2)\n",
        "\\]\n",
        "\\[\n",
        "= 1 - (0.64 + 0.04)\n",
        "\\]\n",
        "\\[\n",
        "= 1 - 0.68 = 0.32\n",
        "\\]\n",
        "\n",
        "This means the impurity of this node is **0.32**, meaning it is still somewhat mixed but not entirely.\n",
        "\n"
      ],
      "metadata": {
        "id": "rJ3Sa0mrEUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 What is the mathematical formula for Entropy\n",
        "\n",
        "#Ans  ### **Mathematical Formula for Entropy**\n",
        "\n",
        "Entropy is a measure of impurity or uncertainty in a dataset. It quantifies the randomness in the\n",
        " distribution of classes at a given node in a decision tree.\n",
        "\n",
        "#### **Formula:**\n",
        "\\[\n",
        "Entropy = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( c \\) = Total number of classes\n",
        "- \\( p_i \\) = Proportion (probability) of class \\( i \\) in the node\n",
        "- \\( \\log_2 \\) = Logarithm to base 2\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation:**\n",
        "Suppose a node contains two classes:\n",
        "- Class A: 80% (\\( p_A = 0.8 \\))\n",
        "- Class B: 20% (\\( p_B = 0.2 \\))\n",
        "\n",
        "\\[\n",
        "Entropy = - (0.8 \\log_2 0.8 + 0.2 \\log_2 0.2)\n",
        "\\]\n",
        "\n",
        "Using logarithm values:\n",
        "\\[\n",
        "\\log_2 0.8 \\approx -0.3219, \\quad \\log_2 0.2 \\approx -2.3219\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "Entropy = - [ (0.8 \\times -0.3219) + (0.2 \\times -2.3219) ]\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= - [ -0.2575 - 0.4644 ] = 0.722\n",
        "\\]\n",
        "\n",
        "So, the entropy for this node is **0.722**, indicating some impurity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Entropy Interpretation:**\n",
        "- \\( Entropy = 0 \\) ‚Üí Pure node (only one class present)\n",
        "- \\( Entropy = 1 \\) ‚Üí Maximum impurity (equal class distribution)\n",
        "- Higher entropy means more disorder and greater impurity.\n"
      ],
      "metadata": {
        "id": "c_Sd4kphEUR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 What is Information Gain, and how is it used in Decision Trees\n",
        "\n",
        "#Ans ### **Information Gain in Decision Trees**\n",
        "\n",
        "**Information Gain (IG)** is a measure of how much uncertainty (entropy) is reduced after a dataset is\n",
        " split based on a feature. It helps in selecting the best feature to split a node in a Decision Tree.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Formula for Information Gain**\n",
        "\n",
        "\\[\n",
        "IG = Entropy(Parent) - \\sum_{k=1}^{m} \\frac{N_k}{N} \\cdot Entropy(Child_k)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( Entropy(Parent) \\) = Entropy of the original dataset before the split\n",
        "- \\( m \\) = Number of subsets after the split\n",
        "- \\( N_k \\) = Number of samples in child node \\( k \\)\n",
        "- \\( N \\) = Total number of samples in the parent node\n",
        "- \\( Entropy(Child_k) \\) = Entropy of the \\( k^{th} \\) child node\n",
        "\n",
        "---\n",
        "\n",
        "### **How Information Gain is Used in Decision Trees**\n",
        "1. **Calculate the Entropy of the Parent Node:**\n",
        "   - Before splitting, measure how mixed the classes are in the dataset.\n",
        "\n",
        "2. **Split the Dataset Based on a Feature:**\n",
        "   - Divide the dataset into subsets using a feature (e.g., \"Income: High/Low\").\n",
        "\n",
        "3. **Compute the Weighted Entropy of Child Nodes:**\n",
        "   - Find the entropy of each subset and weight it by the proportion of samples in that subset.\n",
        "\n",
        "4. **Compute Information Gain:**\n",
        "   - Subtract the weighted child entropies from the parent entropy.\n",
        "\n",
        "5. **Select the Best Feature for Splitting:**\n",
        "   - The feature with the **highest Information Gain** is chosen for the split.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**\n",
        "Consider a dataset where the **Parent Node** has two classes:\n",
        "- **Class A:** 60% (\\( p_A = 0.6 \\))\n",
        "- **Class B:** 40% (\\( p_B = 0.4 \\))\n",
        "\n",
        "#### **Step 1: Compute Parent Entropy**\n",
        "\\[\n",
        "Entropy_{parent} = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\n",
        "\\]\n",
        "\\[\n",
        "= - [0.6(-0.737) + 0.4(-1.322)]\n",
        "\\]\n",
        "\\[\n",
        "= - [-0.442 + (-0.529)] = 0.971\n",
        "\\]\n",
        "\n",
        "#### **Step 2: Split the Data Based on a Feature (e.g., \"Age < 30?\")**\n",
        "- **Left Node (Yes: 4 samples ‚Üí 3 A, 1 B)**\n",
        "  \\[\n",
        "  Entropy_{left} = - \\left( \\frac{3}{4} \\log_2 \\frac{3}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right) = 0.811\n",
        "  \\]\n",
        "- **Right Node (No: 6 samples ‚Üí 3 A, 3 B)**\n",
        "  \\[\n",
        "  Entropy_{right} = - \\left( \\frac{3}{6} \\log_2 \\frac{3}{6} + \\frac{3}{6} \\log_2 \\frac{3}{6} \\right) = 1.0\n",
        "  \\]\n",
        "\n",
        "#### **Step 3: Compute Weighted Entropy of Children**\n",
        "\\[\n",
        "Entropy_{children} = \\left( \\frac{4}{10} \\times 0.811 \\right) + \\left( \\frac{6}{10} \\times 1.0 \\right)\n",
        "\\]\n",
        "\\[\n",
        "= 0.3244 + 0.6 = 0.9244\n",
        "\\]\n",
        "\n",
        "#### **Step 4: Compute Information Gain**\n",
        "\\[\n",
        "IG = 0.971 - 0.9244 = 0.0466\n",
        "\\]\n",
        "\n",
        "Since this IG is low, we might look for another feature with higher IG.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Insights**\n",
        "- **Higher Information Gain ‚Üí Better Split**\n",
        "- If Information Gain is **0**, the feature does not help in classification.\n",
        "- Decision Trees keep selecting the feature with the highest IG at each step.\n",
        "\n"
      ],
      "metadata": {
        "id": "zLRTiCiiEUUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 What is the difference between Gini Impurity and Entropy\n",
        "\n",
        "#ans  ### **Difference Between Gini Impurity and Entropy**\n",
        "\n",
        "Gini Impurity and Entropy are both impurity measures used in Decision Trees to determine the best feature for splitting.\n",
        " However, they differ in their calculations and interpretations.\n",
        "\n",
        "| **Criteria**      | **Gini Impurity** | **Entropy** |\n",
        "|------------------|----------------|------------|\n",
        "| **Formula** | \\[ Gini = 1 - \\sum p_i^2 \\] | \\[ Entropy = -\\sum p_i \\log_2 p_i \\] |\n",
        "| **Range** | \\( [0, 0.5] \\) for binary classification | \\( [0,1] \\) for binary classification |\n",
        "| **Meaning** | Probability of randomly misclassifying an instance | Measure of uncertainty or randomness |\n",
        "| **Computational Complexity** | Faster (no logarithm calculation) | Slower (logarithm computation) |\n",
        "| **Preference** | Used in **CART (Classification and Regression Trees)** | Used in **ID3 and C4.5 algorithms** |\n",
        "| **Splitting Criterion** | Chooses the split that minimizes Gini Impurity | Chooses the split that maximizes Information Gain |\n",
        "| **Bias in Splitting** | Prefers features with **more distinct classes** | Prefers balanced splits with **more uniform distributions** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Comparison**\n",
        "Suppose a node has two classes:\n",
        "- **Class A:** 80% (\\( p_A = 0.8 \\))\n",
        "- **Class B:** 20% (\\( p_B = 0.2 \\))\n",
        "\n",
        "#### **Gini Impurity Calculation**\n",
        "\\[\n",
        "Gini = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32\n",
        "\\]\n",
        "\n",
        "#### **Entropy Calculation**\n",
        "\\[\n",
        "Entropy = - (0.8 \\log_2 0.8 + 0.2 \\log_2 0.2)\n",
        "\\]\n",
        "\\[\n",
        "= - (0.8 \\times -0.3219 + 0.2 \\times -2.3219)\n",
        "\\]\n",
        "\\[\n",
        "= 0.722\n",
        "\\]\n",
        "\n",
        "### **Key Takeaways**\n",
        "- Gini is slightly faster to compute and preferred in **CART**.\n",
        "- Entropy is more sensitive to class distribution and used in **ID3/C4.5**.\n",
        "- In practice, both measures often lead to **similar tree structures**.\n"
      ],
      "metadata": {
        "id": "encwdiWTEUXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 What is the mathematical explanation behind Decision Trees\n",
        "\n",
        "#Ans ### **Mathematical Explanation Behind Decision Trees**\n",
        "\n",
        "A **Decision Tree** is a recursive partitioning algorithm that splits data into subsets based on feature\n",
        " values to minimize impurity. It works by selecting the best feature at each step, using impurity measures like\n",
        " **Gini Impurity**, **Entropy**, or **Mean Squared Error (MSE)** (for regression).\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Splitting Criterion (Choosing the Best Feature)**\n",
        "A feature is chosen based on how well it separates the data into pure groups. The common splitting criteria are:\n",
        "- **Classification:** Information Gain (Entropy) or Gini Impurity\n",
        "- **Regression:** Reduction in Variance (MSE)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Information Gain (Entropy-Based Splitting)**\n",
        "Entropy measures uncertainty in a node:\n",
        "\n",
        "\\[\n",
        "Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2 p_i\n",
        "\\]\n",
        "\n",
        "where \\( p_i \\) is the probability of class \\( i \\) in node \\( S \\).\n",
        "\n",
        "**Information Gain (IG) is the reduction in entropy after splitting:**\n",
        "\\[\n",
        "IG = Entropy(Parent) - \\sum_{k=1}^{m} \\frac{N_k}{N} Entropy(Child_k)\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( N_k \\) is the number of samples in child \\( k \\)\n",
        "- \\( N \\) is the total number of samples in the parent node\n",
        "\n",
        "A feature with the **highest IG** is selected for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Gini Impurity-Based Splitting**\n",
        "Instead of entropy, we can use **Gini Impurity**:\n",
        "\n",
        "\\[\n",
        "Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "\\]\n",
        "\n",
        "The feature with the **lowest Gini Impurity** is chosen.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Regression Tree (MSE-Based Splitting)**\n",
        "For regression problems, Decision Trees minimize the variance of target values:\n",
        "\n",
        "\\[\n",
        "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n",
        "\\]\n",
        "\n",
        "where \\( \\hat{y} \\) is the mean of the target variable in a node.\n",
        "\n",
        "**Reduction in Variance:**\n",
        "\\[\n",
        "\\Delta Variance = Variance(Parent) - \\sum_{k=1}^{m} \\frac{N_k}{N} Variance(Child_k)\n",
        "\\]\n",
        "\n",
        "The feature that **reduces variance the most** is chosen for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Stopping Criteria (Tree Growth Control)**\n",
        "A tree stops growing when:\n",
        "- Maximum depth is reached.\n",
        "- Minimum number of samples per node is met.\n",
        "- Further splits do not significantly reduce impurity.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pruning (Avoiding Overfitting)**\n",
        "- **Pre-pruning:** Stop splitting early based on constraints (e.g., max depth).\n",
        "- **Post-pruning:** Grow a full tree and then remove branches that don't improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Example**\n",
        "#### **Step 1: Compute Parent Entropy**\n",
        "For a dataset with 60% Class A (\\( p_A = 0.6 \\)) and 40% Class B (\\( p_B = 0.4 \\)):\n",
        "\n",
        "\\[\n",
        "Entropy_{parent} = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4) = 0.971\n",
        "\\]\n",
        "\n",
        "#### **Step 2: Compute Entropy After Split**\n",
        "If splitting results in two child nodes:\n",
        "- Left: 75% A, 25% B ‚Üí \\( Entropy_{left} = 0.811 \\)\n",
        "- Right: 50% A, 50% B ‚Üí \\( Entropy_{right} = 1.0 \\)\n",
        "\n",
        "#### **Step 3: Compute Weighted Entropy**\n",
        "\\[\n",
        "Entropy_{children} = \\frac{4}{10} \\times 0.811 + \\frac{6}{10} \\times 1.0 = 0.924\n",
        "\\]\n",
        "\n",
        "#### **Step 4: Compute Information Gain**\n",
        "\\[\n",
        "IG = 0.971 - 0.924 = 0.047\n",
        "\\]\n",
        "\n",
        "Since IG is low, we try another feature.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Decision Trees iteratively select the feature that minimizes impurity or variance, split data accordingly,\n",
        " and stop when further splits are unnecessary."
      ],
      "metadata": {
        "id": "yroagpAFEUaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 What is Pre-Pruning in Decision Trees\n",
        "\n",
        "#Ans  ### **Pre-Pruning in Decision Trees**\n",
        "\n",
        "**Pre-Pruning (Early Stopping)** is a technique used to prevent a Decision Tree from growing too deep and\n",
        "overfitting the training data. It stops the tree **before** it becomes too complex by applying constraints during training.\n",
        "\n",
        "---\n",
        "\n",
        "## **How Pre-Pruning Works**\n",
        "\n",
        "Instead of allowing the tree to grow until all leaves are pure (which may cause overfitting),\n",
        "pre-pruning stops the growth based on one or more stopping conditions.\n",
        "\n",
        "### **Common Stopping Conditions:**\n",
        "1. **Maximum Depth (\\( d_{max} \\))**\n",
        "   - Stops splitting when the tree reaches a certain depth.\n",
        "   - Example: Stop splitting when depth = 5.\n",
        "\n",
        "2. **Minimum Samples per Split (\\( n_{min} \\))**\n",
        "   - Stops splitting if a node has fewer than a threshold number of samples.\n",
        "   - Example: Stop if a node has < 10 samples.\n",
        "\n",
        "3. **Minimum Information Gain (or Gini Reduction)**\n",
        "   - Stops splitting if the improvement in Information Gain or Gini Impurity is below a threshold.\n",
        "   - Example: Stop if Information Gain < 0.01.\n",
        "\n",
        "4. **Maximum Number of Leaves (\\( l_{max} \\))**\n",
        "   - Limits the number of leaf nodes to avoid over-complexity.\n",
        "   - Example: Allow at most 20 leaf nodes.\n",
        "\n",
        "5. **Maximum Impurity Decrease**\n",
        "   - Stops splitting when the decrease in impurity (entropy or Gini) is below a threshold.\n",
        "   - Example: If the impurity decrease is < 0.001, stop splitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Pre-Pruning**\n",
        "‚úÖ **Prevents Overfitting** ‚Äì Avoids learning noise by limiting tree complexity.\n",
        "‚úÖ **Improves Efficiency** ‚Äì Reduces training time by stopping unnecessary splits.\n",
        "‚úÖ **Enhances Interpretability** ‚Äì Produces smaller, more understandable trees.\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages of Pre-Pruning**\n",
        "‚ùå **Risk of Underfitting** ‚Äì The tree may stop growing too early and fail to capture important patterns.\n",
        "‚ùå **Difficult to Set Thresholds** ‚Äì Choosing the right stopping conditions requires tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## **Example in Scikit-Learn (Python)**\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree with Pre-Pruning\n",
        "clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5)\n",
        "clf.fit(X_train, y_train)\n",
        "```\n",
        "This tree:\n",
        "- Stops growing at depth **5**.\n",
        "- Splits a node **only if** it has **at least 10 samples**.\n",
        "- Ensures that **leaf nodes** have at least **5 samples**.\n"
      ],
      "metadata": {
        "id": "Sc9vlAfGEUc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9 What is Post-Pruning in Decision Trees\n",
        "\n",
        "#ans ### **Post-Pruning in Decision Trees**\n",
        "\n",
        "**Post-Pruning (or Pruning after Training)** is a technique used to simplify an **overgrown**\n",
        " Decision Tree by removing unnecessary branches **after** it has been fully grown. This helps reduce **overfitting** and improves generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## **How Post-Pruning Works**\n",
        "1. **Grow a Fully Expanded Decision Tree**\n",
        "   - The tree is allowed to grow **until all nodes are pure** or meet the stopping condition.\n",
        "\n",
        "2. **Evaluate Each Subtree**\n",
        "   - Remove branches that do **not significantly** improve accuracy.\n",
        "   - The pruning is based on a validation dataset or a statistical test.\n",
        "\n",
        "3. **Replace Pruned Nodes with Leaf Nodes**\n",
        "   - The subtree is replaced with a single leaf node representing the most frequent class\n",
        "    (for classification) or the mean value (for regression).\n",
        "\n",
        "---\n",
        "\n",
        "## **Methods of Post-Pruning**\n",
        "\n",
        "### **1. Cost Complexity Pruning (CCP) ‚Äì Used in CART Algorithm**\n",
        "- Introduces a **pruning parameter \\( \\alpha \\)** that controls complexity:\n",
        "  \\[\n",
        "  R(T) = R(T_{orig}) + \\alpha \\times |T|\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( R(T) \\) = Total cost of the tree\n",
        "  - \\( R(T_{orig}) \\) = Error before pruning\n",
        "  - \\( |T| \\) = Number of leaf nodes\n",
        "  - \\( \\alpha \\) = Complexity parameter (higher values prune more aggressively)\n",
        "\n",
        "- The best \\( \\alpha \\) is chosen using **cross-validation**.\n",
        "\n",
        "### **2. Reduced Error Pruning (REP) ‚Äì Used in ID3 Algorithm**\n",
        "- **Removes a subtree** if it does **not reduce error** on the validation set.\n",
        "- If the accuracy **does not drop**, the branch is replaced by a leaf.\n",
        "- Works best for **small datasets** but may not be as effective for large ones.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Post-Pruning**\n",
        "‚úÖ **Prevents Overfitting** ‚Äì Reduces tree complexity after training.\n",
        "‚úÖ **More Reliable than Pre-Pruning** ‚Äì Uses actual data to prune instead of fixed constraints.\n",
        "‚úÖ **Improves Generalization** ‚Äì Produces a model that works well on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages of Post-Pruning**\n",
        "‚ùå **Computationally Expensive** ‚Äì Requires growing a full tree first and then pruning it.\n",
        "‚ùå **Validation Data Needed** ‚Äì Needs extra data to check pruning effectiveness.\n",
        "\n",
        "---\n",
        "\n",
        "## **Example in Scikit-Learn (Python)**\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train a fully grown tree\n",
        "clf = DecisionTreeClassifier(ccp_alpha=0.01)  # Cost Complexity Pruning\n",
        "clf.fit(X_train, y_train)\n",
        "```\n",
        "- **`ccp_alpha=0.01`** controls pruning strength.\n",
        "- **Higher values** of `ccp_alpha` prune more aggressively.\n"
      ],
      "metadata": {
        "id": "_sD9xUqGEUf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. What is the difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "#Ans  ### **Difference Between Pre-Pruning and Post-Pruning**\n",
        "\n",
        "Pre-Pruning and Post-Pruning are techniques used in **Decision Trees** to prevent overfitting by controlling tree complexity.\n",
        " The key difference is **when** the pruning occurs.\n",
        "\n",
        "| Feature        | **Pre-Pruning (Early Stopping)** | **Post-Pruning (Prune After Training)** |\n",
        "|--------------|--------------------------------|--------------------------------|\n",
        "| **When It Happens** | During tree construction | After the tree is fully grown |\n",
        "| **How It Works** | Stops splitting early based on conditions like max depth, min samples per node, or minimum information gain |\n",
        "| **Stopping Criteria** | - Maximum tree depth<br>- Minimum samples per split/leaf<br>- Minimum impurity reduction\n",
        "| **Risk** | May **underfit** by stopping too early | May **overfit initially**, but pruning corrects it |\n",
        "| **Computational Cost** | Faster (stops early) | Slower (tree is fully grown first) |\n",
        "| **Common Algorithms** | Used in **CART, ID3, C4.5** | Used in **CART (Cost Complexity Pruning), ID3 (Reduced Error Pruning)** |\n",
        "| **Example in Python (Scikit-Learn)** | `DecisionTreeClassifier(max_depth=5, min_samples_split=10)` |\n",
        "\n",
        "---\n",
        "\n",
        "### **Which One to Use?**\n",
        "- **Pre-Pruning** is better when you need **faster training** and can tune stopping conditions.\n",
        "- **Post-Pruning** is better when you want a **more optimized tree**."
      ],
      "metadata": {
        "id": "arU-nDhREUtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 What is a Decision Tree Regressor\t.\n",
        "\n",
        "#Ans  ### **Decision Tree Regressor**\n",
        "\n",
        "A **Decision Tree Regressor** is a type of **Decision Tree** used for **regression tasks**, where the\n",
        " target variable is continuous (numerical), rather than categorical (as in classification). Instead of\n",
        " predicting classes, it predicts **numeric values** by recursively splitting the data and computing the average of\n",
        "  target values in each leaf node.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. How Decision Tree Regression Works**\n",
        "1. **Select the Best Feature to Split:**\n",
        "   - Uses a **variance reduction** criterion instead of Gini or Entropy.\n",
        "2. **Recursively Split Data:**\n",
        "   - Creates branches where data is split to minimize error.\n",
        "3. **Stopping Condition:**\n",
        "   - Stops when a predefined depth is reached, or when further splitting does not significantly reduce error.\n",
        "4. **Prediction:**\n",
        "   - For a given input, it follows the decision rules down to a leaf node and returns the **mean target value** of that leaf.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Splitting Criterion: Mean Squared Error (MSE)**\n",
        "Decision Tree Regression uses **MSE** to determine the best splits:\n",
        "\n",
        "\\[\n",
        "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( y_i \\) are actual values,\n",
        "- \\( \\bar{y} \\) is the mean of target values in that node,\n",
        "- \\( N \\) is the number of samples in the node.\n",
        "\n",
        "The **feature that minimizes the weighted MSE** across child nodes is chosen for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Example in Python (Scikit-Learn)**\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([2.5, 3.0, 3.7, 4.5, 5.1, 5.9, 6.8, 7.4, 8.0, 9.2])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(max_depth=3)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Advantages of Decision Tree Regressor**\n",
        "‚úÖ **Handles Non-Linear Data** ‚Äì Works well when relationships are non-linear.\n",
        "‚úÖ **Easy to Interpret** ‚Äì The tree structure is readable.\n",
        "‚úÖ **Handles Missing Data** ‚Äì Can work with missing values.\n",
        "\n",
        "## **5. Disadvantages**\n",
        "‚ùå **Prone to Overfitting** ‚Äì Needs pruning or constraints.\n",
        "‚ùå **Not Smooth Predictions** ‚Äì Step-like predictions instead of continuous curves.\n",
        "‚ùå **Sensitive to Small Changes in Data** ‚Äì Small variations can lead to a different tree structure.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- If your target variable is **continuous**, use a **Decision Tree Regressor**.\n",
        "- It splits data using **variance reduction (MSE)** instead of entropy/Gini.\n",
        "- To prevent overfitting, use **max_depth, min_samples_split, or post-pruning**.\n"
      ],
      "metadata": {
        "id": "9XEHy0zWEUy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 What are the advantages and disadvantages of Decision Trees\n",
        "\n",
        "#Ans  ### **Advantages and Disadvantages of Decision Trees**\n",
        "\n",
        "Decision Trees are a popular machine learning model due to their simplicity and interpretability.\n",
        " However, they also have some limitations.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Decision Trees** ‚úÖ\n",
        "\n",
        "### **1. Easy to Understand and Interpret**\n",
        "- The tree structure makes it intuitive and **visually interpretable**.\n",
        "- Even non-technical users can follow the decision-making process.\n",
        "\n",
        "### **2. Handles Both Classification and Regression**\n",
        "- Works for both **categorical** (classification) and **numerical** (regression) problems.\n",
        "\n",
        "### **3. No Need for Feature Scaling**\n",
        "- Unlike models like **SVM** or **Logistic Regression**, Decision Trees do **not** require normalization or standardization.\n",
        "\n",
        "### **4. Handles Non-Linear Data Well**\n",
        "- Can capture **complex relationships** and interactions between features.\n",
        "- Works well for **non-linear** datasets.\n",
        "\n",
        "### **5. Can Handle Missing Values**\n",
        "- Decision Trees **can work** with missing data by using **surrogate splits**.\n",
        "\n",
        "### **6. Works with Both Small and Large Datasets**\n",
        "- Can be used effectively on **small datasets** where other models might overcomplicate things.\n",
        "\n",
        "### **7. Feature Selection is Automatic**\n",
        "- The algorithm **automatically selects the most important features** for splitting.\n",
        "\n",
        "### **8. Can Handle Multi-Class Classification**\n",
        "- Unlike logistic regression (which is inherently binary), Decision Trees can classify into multiple categories directly.\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages of Decision Trees** ‚ùå\n",
        "\n",
        "### **1. Prone to Overfitting**\n",
        "- If **fully grown**, the tree **memorizes training data**, leading to poor generalization.\n",
        "- **Solution:** Use **pre-pruning** (max depth, min samples per split) or **post-pruning** (Cost Complexity Pruning).\n",
        "\n",
        "### **2. High Variance (Unstable Predictions)**\n",
        "- Small changes in the data can lead to a **completely different tree structure**.\n",
        "- **Solution:** Use **ensemble methods** like **Random Forest** or **Gradient Boosting**.\n",
        "\n",
        "### **3. Not Ideal for Continuous Data Prediction**\n",
        "- In **regression tasks**, the predictions are **step-like** instead of smooth.\n",
        "- **Solution:** Use **Random Forest Regressor** or **pruning**.\n",
        "\n",
        "### **4. Biased Splitting (Favours Dominant Features)**\n",
        "- If a feature has **more unique values**, the tree may **prefer it** for splitting (even if it‚Äôs not the most important).\n",
        "- **Solution:** Use **feature selection techniques**.\n",
        "\n",
        "### **5. Computational Cost for Deep Trees**\n",
        "- If the dataset is large and the tree is deep, training can be slow.\n",
        "- **Solution:** Use **Random Forest** (which limits tree depth) or **Gradient Boosting**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary Table: Pros & Cons**\n",
        "\n",
        "| **Feature**               | **Advantage** ‚úÖ  | **Disadvantage** ‚ùå  |\n",
        "|----------------------|------------------|------------------|\n",
        "| **Interpretability** | Easy to understand | Can be too complex when deep |\n",
        "| **Feature Scaling** | Not needed | - |\n",
        "| **Handling Non-Linearity** | Works well | Overfits without pruning |\n",
        "| **Overfitting** | - | Prone to overfitting |\n",
        "| **Computation Time** | Fast for small data | Slow for deep trees |\n",
        "| **Data Sensitivity** | Captures interactions well | High variance (unstable predictions) |\n",
        "| **Handling Missing Data** | Can handle missing values | - |\n",
        "| **Continuous Data Prediction** | Works, but step-like | Not smooth |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Decision Trees?**\n",
        "‚úÖ When **interpretability** is important.\n",
        "‚úÖ When the dataset has **non-linear relationships**.\n",
        "‚úÖ When you need **feature selection automatically**.\n",
        "‚úÖ When working with **small to medium-sized datasets**.\n",
        "\n",
        "üö´ Avoid Decision Trees if:\n",
        "‚ùå You need a **highly stable model** ‚Üí Use **Random Forest**.\n",
        "‚ùå You have **very large datasets** ‚Üí Use **Gradient Boosting**.\n",
        "‚ùå You need smooth predictions for regression ‚Üí Use **Linear Regression or Neural Networks**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2zOKIX9aEU1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 How does a Decision Tree handle missing values\n",
        "\n",
        "#ANs   ### **How Decision Trees Handle Missing Values**\n",
        "\n",
        "Decision Trees can handle missing values in both **features (input data)** and **target variables (output data)** in several ways.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Handling Missing Values in Features (Input Data)**\n",
        "When some feature values are missing, Decision Trees handle them in the following ways:\n",
        "\n",
        "### **A. Ignoring Missing Values (Default in Scikit-Learn)**\n",
        "- If a row has a missing value in a feature, **it is ignored for that particular split**.\n",
        "- However, the row is still used for splits on other features.\n",
        "\n",
        "### **B. Surrogate Splitting (Used in CART Algorithm)**\n",
        "- If a feature has missing values, the algorithm finds an **alternative (surrogate) feature** that best mimics\n",
        "the original feature‚Äôs split.\n",
        "- The missing values are then assigned based on this surrogate feature.\n",
        "\n",
        "### **C. Assigning to the Most Common Split (Mode or Mean Imputation)**\n",
        "- For **categorical features**, missing values are assigned to the **most frequent category**.\n",
        "- For **numerical features**, missing values are assigned to the **mean or median** of the feature.\n",
        "\n",
        "### **D. Using \"Missing\" as a Separate Category (for Categorical Features)**\n",
        "- If a categorical feature has missing values, some implementations create a new category called **\"Missing\"**\n",
        "and treat it as a separate class.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Handling Missing Values in Target Variables (Output Data)**\n",
        "- If the target variable (**Y**) has missing values, those rows are usually **dropped** during training.\n",
        "- Alternatively, missing target values can be imputed using techniques like **mean imputation** (for regression)\n",
        " or **mode imputation** (for classification).\n",
        "\n",
        "---\n",
        "\n",
        "## **Example in Scikit-Learn**\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create dataset with missing values\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': [1, 2, np.nan, 4, 5],\n",
        "    'Feature2': [3, np.nan, 1, 2, 5],\n",
        "    'Target': [0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "# Split into X (features) and y (target)\n",
        "X = data[['Feature1', 'Feature2']]\n",
        "y = data['Target']\n",
        "\n",
        "# Impute missing values using mean strategy\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_imputed, y)\n",
        "```\n",
        "**Here‚Äôs what happens:**\n",
        "- We replace missing values with the **mean** before training the model.\n",
        "- The Decision Tree then learns from the cleaned dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Decision Trees in Handling Missing Data**\n",
        "‚úÖ **Can work with missing values without imputation** (e.g., surrogate splits).\n",
        "‚úÖ **Does not require feature scaling**, making it simpler to use.\n",
        "‚úÖ **Robust to missing values**, unlike algorithms like SVM or Logistic Regression.\n",
        "\n",
        "## **Disadvantages**\n",
        "‚ùå **Surrogate splits may not always be reliable**, especially in small datasets.\n",
        "‚ùå **Imputation may introduce bias**, especially if missing data is **not random**.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Different Methods?**\n",
        "- **If missing values are few** ‚Üí Use **mean/median imputation**.\n",
        "- **If many missing values exist** ‚Üí Use **surrogate splitting** (available in some libraries).\n",
        "- **For categorical features** ‚Üí Treat \"missing\" as a separate category.\n"
      ],
      "metadata": {
        "id": "hL4M6XYWEU4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 * How does a Decision Tree handle categorical features\n",
        "\n",
        "#Ans  ### **How Decision Trees Handle Categorical Features**\n",
        "\n",
        "Decision Trees can **naturally handle categorical features** without needing to convert them into\n",
        "numerical values explicitly. However, different algorithms and implementations handle categorical features differently.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Methods for Handling Categorical Features**\n",
        "\n",
        "### **A. Using One-Hot Encoding (Most Common in Scikit-Learn)**\n",
        "- Converts each category into a separate binary column (0 or 1).\n",
        "- Works well when there are **few categories** but can cause high-dimensional data for many categories.\n",
        "\n",
        "#### **Example: One-Hot Encoding in Scikit-Learn**\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([['Red'], ['Blue'], ['Green'], ['Red'], ['Green']])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_encoded, y)\n",
        "```\n",
        "‚úÖ **Pros:** Works well for small categorical feature sets.\n",
        "‚ùå **Cons:** Can lead to the **curse of dimensionality** if too many categories exist.\n",
        "\n",
        "---\n",
        "\n",
        "### **B. Using Label Encoding (For Ordered Categories)**\n",
        "- Assigns a unique number to each category.\n",
        "- **Works best when categories have a meaningful order** (e.g., **Low = 0, Medium = 1, High = 2**).\n",
        "- If there‚Äôs **no natural order**, label encoding can mislead the tree.\n",
        "\n",
        "#### **Example: Label Encoding in Scikit-Learn**\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example categorical feature\n",
        "X = np.array([['Red'], ['Blue'], ['Green'], ['Red'], ['Green']])\n",
        "\n",
        "# Apply Label Encoding\n",
        "encoder = LabelEncoder()\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "print(X_encoded)  # Output: [2, 0, 1, 2, 1]\n",
        "```\n",
        "‚úÖ **Pros:** Simple and works for ordinal categories.\n",
        "‚ùå **Cons:** Can mislead the model if categories have no order.\n",
        "\n",
        "---\n",
        "\n",
        "### **C. Using Decision Trees That Natively Handle Categorical Data (e.g., XGBoost, LightGBM, H2O,\n",
        " and Scikit-Learn's `DecisionTreeClassifier` with `dtype=\"category\"`)**\n",
        "- Some libraries can **directly handle categorical features** without encoding.\n",
        "- **LightGBM** and **H2O.ai** are optimized for categorical variables.\n",
        "\n",
        "#### **Example: LightGBM Handling Categorical Features Natively**\n",
        "```python\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "# Example categorical dataset\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Green'], 'Target': [0, 1, 0, 1, 0]})\n",
        "df['Color'] = df['Color'].astype('category')  # Convert to categorical type\n",
        "\n",
        "# Train LightGBM Model\n",
        "model = lgb.LGBMClassifier()\n",
        "model.fit(df[['Color']], df['Target'])\n",
        "```\n",
        "‚úÖ **Pros:** Faster and more efficient than one-hot encoding.\n",
        "‚ùå **Cons:** Not available in basic Decision Tree implementations like Scikit-Learn.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How Splitting Works on Categorical Features**\n",
        "\n",
        "- **For binary splits (CART Algorithm, used in Scikit-Learn)**:\n",
        "  - The tree groups categories into **two subsets** (e.g., `{Red, Green}` vs. `{Blue}`) and selects the best split.\n",
        "\n",
        "- **For multi-way splits (ID3, C4.5, and CHAID Algorithms)**:\n",
        "  - The tree creates a separate branch for **each category**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Best Practices for Handling Categorical Features in Decision Trees**\n",
        "| **Scenario** | **Best Encoding Method** |\n",
        "|-------------|-------------------------|\n",
        "| Few categories (< 10) | One-Hot Encoding |\n",
        "| Many categories (> 10) | Use Label Encoding or LightGBM |\n",
        "| Ordinal categories (Low, Medium, High) | Label Encoding |\n",
        "| Large datasets with categorical data | Use LightGBM or H2O.ai (native support) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- **Scikit-Learn‚Äôs Decision Trees require encoding (One-Hot or Label Encoding).**\n",
        "- **Advanced libraries (LightGBM, H2O) handle categorical data natively** for better performance.\n",
        "- **Choose encoding wisely:** One-Hot for small categories, Label Encoding for ordered categories, and native handling for large datasets.\n"
      ],
      "metadata": {
        "id": "A2FUlg7KEU7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15   What are some real-world applications of Decision Trees?\n",
        "\n",
        "#Ans  ### **Real-World Applications of Decision Trees** üåçüå≥\n",
        "\n",
        "Decision Trees are widely used in various fields due to their **interpretability**, **efficiency**,\n",
        " and ability to handle both **classification** and **regression** problems. Here are some key real-world applications:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Healthcare & Medical Diagnosis üè•**\n",
        "üîπ **Disease Prediction & Diagnosis**\n",
        "   - Used to classify **patients as high-risk or low-risk** for diseases (e.g., diabetes, heart disease, cancer).\n",
        "   - Example: A Decision Tree can analyze **symptoms, test results, and medical history** to diagnose a disease.\n",
        "\n",
        "üîπ **Treatment Recommendation**\n",
        "   - Helps doctors **choose the best treatment** based on a patient's symptoms and history.\n",
        "\n",
        "üîπ **Predicting Patient Readmission**\n",
        "   - Hospitals use Decision Trees to predict **which patients are likely to be readmitted** based on past records.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Banking & Finance üí∞**\n",
        "üîπ **Credit Risk Assessment**\n",
        "   - Banks use Decision Trees to **approve or reject loan applications** based on income, credit score, and financial history.\n",
        "\n",
        "üîπ **Fraud Detection**\n",
        "   - Identifies fraudulent transactions by analyzing patterns in **transaction history, location, and spending behavior**.\n",
        "\n",
        "üîπ **Stock Market Prediction**\n",
        "   - Used in algorithmic trading to decide **buy/sell strategies** based on market indicators.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. E-Commerce & Retail üõí**\n",
        "üîπ **Customer Segmentation & Recommendation Systems**\n",
        "   - Helps classify customers based on purchasing behavior to recommend **personalized products**.\n",
        "   - Example: Amazon uses Decision Trees to **suggest products** based on a customer‚Äôs purchase history.\n",
        "\n",
        "üîπ **Churn Prediction**\n",
        "   - Identifies customers likely to stop using a service (churn) based on past interactions and purchase history.\n",
        "   - Example: Subscription services (Netflix, Spotify) use Decision Trees to predict **which users might cancel** their subscriptions.\n",
        "\n",
        "üîπ **Pricing Optimization**\n",
        "   - Helps businesses decide **optimal prices** based on demand, competition, and seasonal trends.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Manufacturing & Quality Control üè≠**\n",
        "üîπ **Defect Detection**\n",
        "   - Identifies defective products based on sensor data and quality checks.\n",
        "\n",
        "üîπ **Supply Chain Optimization**\n",
        "   - Predicts delays and optimizes logistics by analyzing past supply chain data.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Education & Student Performance üìö**\n",
        "üîπ **Predicting Student Performance**\n",
        "   - Schools use Decision Trees to predict **which students are at risk of failing** based on attendance, test scores, and study habits.\n",
        "\n",
        "üîπ **Personalized Learning Plans**\n",
        "   - Adaptive learning platforms use Decision Trees to recommend **custom study materials** based on student weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Marketing & Advertising üì¢**\n",
        "üîπ **Targeted Advertising**\n",
        "   - Helps classify customers into different **buyer personas** for personalized marketing campaigns.\n",
        "\n",
        "üîπ **Lead Scoring**\n",
        "   - Companies use Decision Trees to predict **which leads are most likely to convert** into customers.\n",
        "\n",
        "üîπ **Email Marketing Optimization**\n",
        "   - Predicts **which email campaigns** will have higher engagement based on user behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Human Resources & Employee Management üë•**\n",
        "üîπ **Employee Attrition Prediction**\n",
        "   - Predicts which employees are likely to leave the company based on work satisfaction, salary, and tenure.\n",
        "\n",
        "üîπ **Hiring Decisions**\n",
        "   - Helps HR teams shortlist candidates based on skills, experience, and interview performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Energy & Utilities ‚ö°**\n",
        "üîπ **Energy Consumption Prediction**\n",
        "   - Predicts **electricity or water consumption** for better resource management.\n",
        "\n",
        "üîπ **Fault Detection in Power Grids**\n",
        "   - Helps detect anomalies and **prevent power outages** by analyzing grid performance data.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Transportation & Logistics üöö**\n",
        "üîπ **Route Optimization**\n",
        "   - Used by logistics companies (FedEx, UPS) to determine the **fastest delivery routes** based on traffic, weather, and past data.\n",
        "\n",
        "üîπ **Predicting Flight Delays**\n",
        "   - Airlines use Decision Trees to predict **flight delays** based on weather conditions and past schedules.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Agriculture & Environmental Science üå±**\n",
        "üîπ **Crop Disease Detection**\n",
        "   - Uses Decision Trees to classify crops based on images and identify **diseases** early.\n",
        "\n",
        "üîπ **Weather Forecasting**\n",
        "   - Helps predict **rainfall, temperature trends, and natural disasters**.\n",
        "\n",
        "üîπ **Soil Quality Assessment**\n",
        "   - Analyzes soil samples to recommend **best crops** for farming.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion: Why Use Decision Trees?**\n",
        "‚úÖ **Easy to interpret** ‚Äì Clear decision-making process.\n",
        "‚úÖ **Handles non-linear data well** ‚Äì Can classify complex patterns.\n",
        "‚úÖ **Works with missing values** ‚Äì Can still make predictions with incomplete data.\n",
        "‚úÖ **Versatile** ‚Äì Used in **finance, healthcare, e-commerce, marketing, and many more fields**.\n",
        "\n"
      ],
      "metadata": {
        "id": "_iDET5ndEU93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                  #PRACTICAL"
      ],
      "metadata": {
        "id": "LBniQWniEVA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy*\n",
        "\n",
        "#ans Here‚Äôs a Python program to train a **Decision Tree Classifier** on the **Iris dataset**\n",
        " and print the **model accuracy**. üöÄ\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** using `sklearn.tree.DecisionTreeClassifier`.\n",
        "4Ô∏è‚É£ Predict the labels on the test set and **calculate accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)  # Create model\n",
        "clf.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")  # Print accuracy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The accuracy should be **above 90%**, typically around **0.93 - 1.00**, since the **Iris dataset is simple**.\n",
        "\n",
        "```\n",
        "Model Accuracy: 0.97\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Tune Hyperparameters** (e.g., `max_depth`, `min_samples_split`).\n",
        "‚úÖ **Visualize the Decision Tree** using `plot_tree()`.\n",
        "‚úÖ **Use Cross-Validation** (`cross_val_score`) for more reliable accuracy.\n"
      ],
      "metadata": {
        "id": "98Y19RxnEVDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances*\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using **Gini Impurity** as the criterion\n",
        " and print the **feature importances**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** with `criterion=\"gini\"`.\n",
        "4Ô∏è‚É£ Print **feature importances** to see which features influence the model the most.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)  # Use \"gini\" as the criterion\n",
        "clf.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The feature importances indicate how much each feature contributes to the Decision Tree‚Äôs splits.\n",
        "\n",
        "Example output:\n",
        "```\n",
        "Feature Importances:\n",
        "sepal length (cm): 0.01\n",
        "sepal width (cm): 0.00\n",
        "petal length (cm): 0.56\n",
        "petal width (cm): 0.43\n",
        "```\n",
        "üöÄ **Petal length & width** are usually the most important features for classifying **Iris species**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Change the `criterion` to `\"entropy\"`** and compare results.\n",
        "‚úÖ **Plot the Decision Tree** using `plot_tree()`.\n",
        "‚úÖ **Tune Hyperparameters** like `max_depth` for better performance.\n"
      ],
      "metadata": {
        "id": "RSSbkytJEVJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18  Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy*\n",
        "\n",
        "#Ans   Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using **Entropy** as the splitting\n",
        " criterion and print the **model accuracy**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** with `criterion=\"entropy\"`.\n",
        "4Ô∏è‚É£ Make predictions and compute **model accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier using Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)  # Use \"entropy\" as the splitting criterion\n",
        "clf.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")  # Print accuracy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The model should achieve **high accuracy (~95-100%)** on the **Iris dataset**, as it's a well-separated dataset.\n",
        "\n",
        "```\n",
        "Model Accuracy: 0.97\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Compare with Gini Impurity (`criterion=\"gini\"`)** to see performance differences.\n",
        "‚úÖ **Visualize the Decision Tree** using `plot_tree()`.\n",
        "‚úÖ **Use Cross-Validation** (`cross_val_score`) for more reliable accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "fAt9zlULEVL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*\n",
        "\n",
        "#Ans   Here‚Äôs a **Python program** to train a **Decision Tree Regressor** on the **California Housing dataset**\n",
        "and evaluate it using **Mean Squared Error (MSE)**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **California Housing dataset** using `sklearn.datasets.fetch_california_housing`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train a **Decision Tree Regressor** using `sklearn.tree.DecisionTreeRegressor`.\n",
        "4Ô∏è‚É£ Predict house prices and compute **Mean Squared Error (MSE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data    # Features (e.g., median income, total rooms, population, etc.)\n",
        "y = housing.target  # Target (median house price)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)  # Create model\n",
        "regressor.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")  # Print MSE\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The **MSE value** will depend on the **depth of the tree**, but typically falls between **0.2 - 0.8** for this dataset.\n",
        "\n",
        "Example output:\n",
        "```\n",
        "Mean Squared Error (MSE): 0.4123\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Tune Hyperparameters** (`max_depth`, `min_samples_split`) for better performance.\n",
        "‚úÖ **Compare with Linear Regression** to see which performs better.\n",
        "‚úÖ **Use Feature Importance (`feature_importances_`)** to analyze key factors.\n"
      ],
      "metadata": {
        "id": "w2YRYlhbEVOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "#ans   Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and visualize the tree using **Graphviz**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Train a **Decision Tree Classifier**.\n",
        "3Ô∏è‚É£ Export the tree using `export_graphviz` and visualize it using **Graphviz**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)  # Use \"gini\" as the criterion\n",
        "clf.fit(X, y)  # Train model\n",
        "\n",
        "# Export the tree to Graphviz format\n",
        "dot_data = export_graphviz(\n",
        "    clf, out_file=None, feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names, filled=True, rounded=True, special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the decision tree using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves the tree as a .pdf file\n",
        "graph.view()  # Opens the visualization\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works:**\n",
        "‚úÖ The decision tree is saved as **\"decision_tree.pdf\"** in the current directory.\n",
        "‚úÖ The `graph.view()` command **opens the tree visualization** automatically.\n",
        "\n",
        "---\n",
        "\n",
        "### **Prerequisites:**\n",
        "Make sure you have **Graphviz installed**. If not, install it using:\n",
        "üîπ **For Python:**\n",
        "```bash\n",
        "pip install graphviz\n",
        "```\n",
        "üîπ **For System:**\n",
        "- **Windows:** Download & install from [Graphviz Official Site](https://graphviz.gitlab.io/download/).\n",
        "- **Mac (Homebrew):**\n",
        "  ```bash\n",
        "  brew install graphviz\n",
        "  ```\n",
        "- **Linux (Ubuntu/Debian):**\n",
        "  ```bash\n",
        "  sudo apt install graphviz\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üöÄ A **colorful decision tree** with labeled nodes, showing how features split the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "siofuMBuEVRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        " accuracy with a fully grown tree*\n",
        "\n",
        "#Ans   Here‚Äôs a **Python program** to train a **Decision Tree Classifier** with a maximum depth of **3** and\n",
        "compare its accuracy with a **fully grown tree**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train two **Decision Tree Classifiers**:\n",
        "   - One with `max_depth=3`.\n",
        "   - One fully grown (default settings).\n",
        "4Ô∏è‚É£ Compare their accuracies.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier with max_depth = 3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The **accuracy of max_depth=3** is usually **slightly lower** than a fully grown tree but helps prevent overfitting.\n",
        "\n",
        "```\n",
        "Accuracy with max_depth=3: 0.97\n",
        "Accuracy with fully grown tree: 1.00\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Plot both trees** using `plot_tree()` or `graphviz`.\n",
        "‚úÖ **Try different depths (`max_depth=2, 4, 5`)** to see accuracy changes.\n",
        "‚úÖ **Use Cross-Validation** for more robust evaluation.\n"
      ],
      "metadata": {
        "id": "sRJ_1n-cEVUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22   Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree*\n",
        "\n",
        "#Ans   Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using `min_samples_split=5` and\n",
        "compare its accuracy with a **default tree**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train two **Decision Tree Classifiers**:\n",
        "   - One with `min_samples_split=5` (to prevent overfitting).\n",
        "   - One with **default settings**.\n",
        "4Ô∏è‚É£ Compare their accuracies.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier with min_samples_split=5\n",
        "clf_limited = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The **tree with `min_samples_split=5`** will have **slightly lower accuracy** than the default tree but may generalize better.\n",
        "\n",
        "```\n",
        "Accuracy with min_samples_split=5: 0.97\n",
        "Accuracy with default tree: 1.00\n",
        "```\n",
        "üöÄ The default tree might **overfit**, while the **limited tree prevents splits on small samples**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Plot both trees** using `plot_tree()` or `graphviz`.\n",
        "‚úÖ **Try different values (`min_samples_split=2, 10, 20`)** to analyze its effect.\n",
        "‚úÖ **Use Cross-Validation (`cross_val_score`)** for more reliable accuracy.\n"
      ],
      "metadata": {
        "id": "DKlry47sEVXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23  * Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to apply **feature scaling** before training a **Decision Tree Classifier**\n",
        " and compare its accuracy with **unscaled data**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Apply Feature Scaling?**\n",
        "Decision Trees **don‚Äôt require feature scaling**, but in some cases (like when using distance-based classifiers or mixed models),\n",
        " it can **affect performance**. This test will help us see if scaling improves accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Apply **feature scaling** using `StandardScaler`.\n",
        "4Ô∏è‚É£ Train two **Decision Tree Classifiers**:\n",
        "   - One on **unscaled data**.\n",
        "   - One on **scaled data**.\n",
        "5Ô∏è‚É£ Compare their **accuracies**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier on unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Decision Tree Classifier on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy without feature scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with feature scaling: {accuracy_scaled:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ **Accuracy remains the same** in most cases because Decision Trees are not sensitive to feature scaling.\n",
        "\n",
        "```\n",
        "Accuracy without feature scaling: 1.00\n",
        "Accuracy with feature scaling: 1.00\n",
        "```\n",
        "üöÄ Feature scaling usually impacts **distance-based models** (like SVM, k-NN), but **not Decision Trees**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Enhancements You Can Try:**\n",
        "‚úÖ **Try different scalers (`MinMaxScaler`, `RobustScaler`)** and see if accuracy changes.\n",
        "‚úÖ **Compare with distance-based models like `KNeighborsClassifier`** to see the effect of scaling.\n",
        "‚úÖ **Test on a different dataset where features have very different scales** (e.g., Boston Housing).\n"
      ],
      "metadata": {
        "id": "JENRwkn5EVaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using the **One-vs-Rest (OvR)\n",
        "strategy** for multiclass classification. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **What is One-vs-Rest (OvR)?**\n",
        "üîπ **One-vs-Rest (OvR)** is a strategy where the classifier trains one model per class, treating it as the\n",
        " **positive class** while all other classes are combined as a **single negative class**.\n",
        "üîπ This is useful for classifiers that are inherently **binary**, but since Decision Trees natively support\n",
        "**multiclass classification**, OvR is not required‚Äîbut we can still explicitly apply it.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Apply **One-vs-Rest (OvR) strategy** using `OneVsRestClassifier`.\n",
        "4Ô∏è‚É£ Train the **Decision Tree Classifier**.\n",
        "5Ô∏è‚É£ Predict and evaluate using **accuracy score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using One-vs-Rest (OvR) strategy\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The accuracy should be **~95-100%** on the **Iris dataset**, as it's well-separated.\n",
        "\n",
        "```\n",
        "Model Accuracy with One-vs-Rest: 0.97\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use One-vs-Rest with Decision Trees?**\n",
        "‚úÖ **Doesn't change much for Decision Trees**, since they already handle multiclass classification.\n",
        "‚úÖ Can be useful when combining Decision Trees with **other models** in an ensemble.\n",
        "\n"
      ],
      "metadata": {
        "id": "YjFPDLKyEVdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25  Write a Python program to train a Decision Tree Classifier and display the feature importance scores*\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and display the **feature importance scores**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **What is Feature Importance?**\n",
        "üîπ **Feature importance** tells us which features are most useful in making decisions within the tree.\n",
        "üîπ Higher values indicate **more important features**.\n",
        "üîπ Helps in **feature selection** and **model interpretability**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** using `sklearn.datasets.load_iris`.\n",
        "2Ô∏è‚É£ Train a **Decision Tree Classifier**.\n",
        "3Ô∏è‚É£ Extract and display **feature importance scores**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features (Sepal length, Sepal width, Petal length, Petal width)\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(feature_importance_df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The output will show **which features are most important** in making decisions.\n",
        "Example output (values may vary):\n",
        "\n",
        "```\n",
        "Feature Importance Scores:\n",
        "           Feature  Importance\n",
        "2  petal length (cm)      0.67\n",
        "3   petal width (cm)      0.29\n",
        "0  sepal length (cm)      0.04\n",
        "1   sepal width (cm)      0.00\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Insights:**\n",
        "‚úÖ **Petal length & petal width** are the most important features in classifying Iris species.\n",
        "‚úÖ **Feature selection** can be used to remove less important features.\n",
        "\n"
      ],
      "metadata": {
        "id": "-ycJ1F92EVgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 * Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*\n",
        "\n",
        "#Ans Here‚Äôs a **Python program** to train a **Decision Tree Regressor** with `max_depth=5` and\n",
        "compare its performance with an **unrestricted tree** using **Mean Squared Error (MSE)**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **California Housing dataset** (or any regression dataset).\n",
        "2Ô∏è‚É£ Split the dataset into **training** and **testing** sets.\n",
        "3Ô∏è‚É£ Train two **Decision Tree Regressors**:\n",
        "   - One with `max_depth=5`.\n",
        "   - One **fully grown** (default settings).\n",
        "4Ô∏è‚É£ Compare their **MSE (Mean Squared Error)** to evaluate performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data    # Features\n",
        "y = housing.target  # Target variable (house price)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth = 5\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train fully grown Decision Tree Regressor\n",
        "reg_full = DecisionTreeRegressor(random_state=42)\n",
        "reg_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Print performance results\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"MSE with fully grown tree: {mse_full:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The **MSE (Mean Squared Error)** will likely be **higher for the limited-depth tree** but will **generalize better**.\n",
        "üîπ The **fully grown tree** may **overfit**, leading to lower training error but possibly worse test error.\n",
        "\n",
        "```\n",
        "MSE with max_depth=5: 0.4321\n",
        "MSE with fully grown tree: 0.2954\n",
        "```\n",
        "*(Values may vary depending on dataset split.)*\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Insights:**\n",
        "‚úÖ **Max-depth control prevents overfitting** and improves generalization.\n",
        "‚úÖ **Fully grown trees may memorize the training data**, reducing test accuracy.\n",
        "‚úÖ **Hyperparameter tuning (`max_depth`, `min_samples_split`)** helps optimize performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ayNbcJGHEVjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 * Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy*\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier**, apply **Cost Complexity Pruning (CCP)**,\n",
        "and visualize its effect on accuracy. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **What is Cost Complexity Pruning (CCP)?**\n",
        "üîπ **CCP (Post-Pruning)** reduces model complexity by **removing splits that have little impact** on accuracy.\n",
        "üîπ Controlled by **ccp_alpha**:\n",
        "   - **Lower Œ±** ‚Üí More splits (complex tree).\n",
        "   - **Higher Œ±** ‚Üí Fewer splits (simpler tree).\n",
        "üîπ Helps prevent **overfitting** and improves **generalization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** and split into training/testing sets.\n",
        "2Ô∏è‚É£ Train an **unpruned Decision Tree** and compute accuracy.\n",
        "3Ô∏è‚É£ Extract **cost complexity pruning path** (`ccp_alphas`).\n",
        "4Ô∏è‚É£ Train multiple trees with different **ccp_alpha** values and measure accuracy.\n",
        "5Ô∏è‚É£ **Plot accuracy vs ccp_alpha** to visualize the effect of pruning.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an initial Decision Tree Classifier (Unpruned)\n",
        "clf_unpruned = DecisionTreeClassifier(random_state=42)\n",
        "clf_unpruned.fit(X_train, y_train)\n",
        "\n",
        "# Compute accuracy for unpruned tree\n",
        "y_pred_unpruned = clf_unpruned.predict(X_test)\n",
        "accuracy_unpruned = accuracy_score(y_test, y_pred_unpruned)\n",
        "\n",
        "# Get cost complexity pruning path\n",
        "path = clf_unpruned.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas[:-1]  # Exclude the last alpha (fully pruned tree)\n",
        "\n",
        "# Train Decision Trees for different values of ccp_alpha\n",
        "accuracy_scores = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Plot Accuracy vs. ccp_alpha\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, accuracy_scores, marker=\"o\", linestyle=\"dashed\", label=\"Pruned Tree\")\n",
        "plt.axhline(y=accuracy_unpruned, color='r', linestyle='--', label=\"Unpruned Accuracy\")\n",
        "plt.xlabel(\"CCP Alpha (Pruning Strength)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output & Insights:**\n",
        "üîπ The **accuracy vs. pruning strength (ccp_alpha)** plot will typically show:\n",
        "- **Small Œ± (left side)** ‚Üí Overfitting (complex tree, high accuracy but may not generalize well).\n",
        "- **Optimal Œ± (middle)** ‚Üí Best balance of accuracy & generalization.\n",
        "- **Large Œ± (right side)** ‚Üí Underfitting (pruned too much, reduced accuracy).\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways:**\n",
        "‚úÖ **Unpruned trees** can **overfit** the training data.\n",
        "‚úÖ **Moderate pruning** can improve **generalization**.\n",
        "‚úÖ **Too much pruning (high ccp_alpha)** can lead to **underfitting**.\n",
        "‚úÖ **Choosing the right Œ±** is crucial‚Äîtry **cross-validation** for tuning.\n"
      ],
      "metadata": {
        "id": "xxbp2zcmEVl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and evaluate its performance\n",
        " using **Precision, Recall, and F1-Score**. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **What are Precision, Recall, and F1-Score?**\n",
        "üîπ **Precision** ‚Üí Measures how many predicted positives are actual positives. (**TP / (TP + FP)**)\n",
        "üîπ **Recall** ‚Üí Measures how many actual positives were correctly predicted. (**TP / (TP + FN)**)\n",
        "üîπ **F1-Score** ‚Üí Harmonic mean of Precision and Recall. **(2 √ó Precision √ó Recall) / (Precision + Recall)**\n",
        "üîπ These are crucial when dealing with **imbalanced datasets**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** and split it into **training/testing sets**.\n",
        "2Ô∏è‚É£ Train a **Decision Tree Classifier**.\n",
        "3Ô∏è‚É£ Predict test labels and compute **Precision, Recall, and F1-Score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print classification report (Precision, Recall, F1-Score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ The **classification report** will display **Precision, Recall, and F1-Score** for each class.\n",
        "\n",
        "```\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00         9\n",
        "  versicolor       1.00      0.91      0.95        11\n",
        "   virginica       0.92      1.00      0.96        10\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "```\n",
        "*(Values may vary depending on dataset split.)*\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways:**\n",
        "‚úÖ **Accuracy isn‚Äôt always enough**; **Precision, Recall, and F1-Score** give deeper insights.\n",
        "‚úÖ Useful for **imbalanced datasets** (e.g., fraud detection, medical diagnosis).\n",
        "‚úÖ **F1-Score balances Precision & Recall**, making it a good overall metric.\n"
      ],
      "metadata": {
        "id": "3l4HqW73EVop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and visualize the **confusion matrix** using **Seaborn**.\n",
        "\n",
        "---\n",
        "\n",
        "### **What is a Confusion Matrix?**\n",
        "A **confusion matrix** helps evaluate classification performance by showing:\n",
        "- **True Positives (TP)** ‚Äì Correctly classified as positive.\n",
        "- **True Negatives (TN)** ‚Äì Correctly classified as negative.\n",
        "- **False Positives (FP)** ‚Äì Incorrectly classified as positive.\n",
        "- **False Negatives (FN)** ‚Äì Incorrectly classified as negative.\n",
        "\n",
        "It‚Äôs useful for understanding **misclassifications** in a model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** and split into **training/testing sets**.\n",
        "2Ô∏è‚É£ Train a **Decision Tree Classifier**.\n",
        "3Ô∏è‚É£ Predict test labels and compute the **confusion matrix**.\n",
        "4Ô∏è‚É£ Visualize the confusion matrix using **Seaborn heatmap**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ A **heatmap** of the confusion matrix where:\n",
        "- Diagonal values ‚Üí Correct classifications ‚úÖ\n",
        "- Off-diagonal values ‚Üí Misclassifications ‚ùå\n",
        "\n",
        "üîπ **Example Confusion Matrix Visualization:**\n",
        "üìä *(Assuming no misclassification, the diagonal has high values while others are zero.)*\n",
        "```\n",
        "          Predicted\n",
        "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        " True    ‚îÇ   9   ‚îÇ   0   ‚îÇ   0   ‚îÇ  (Setosa)\n",
        " Label   ‚îÇ   0   ‚îÇ  10   ‚îÇ   1   ‚îÇ  (Versicolor)\n",
        "         ‚îÇ   0   ‚îÇ   0   ‚îÇ  10   ‚îÇ  (Virginica)\n",
        "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Insights:**\n",
        "‚úÖ **Confusion matrix** helps diagnose classification errors.\n",
        "‚úÖ **Seaborn heatmaps** make it easy to visualize misclassifications.\n",
        "‚úÖ Use **Precision, Recall, and F1-Score** for deeper insights.\n"
      ],
      "metadata": {
        "id": "d1ZajEosEVro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split.\n",
        "\n",
        "#Ans  Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and use **GridSearchCV** to\n",
        "find the **optimal values** for `max_depth` and `min_samples_split`. üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "### **What is GridSearchCV?**\n",
        "üîπ **GridSearchCV** is a **hyperparameter tuning** technique that:\n",
        "- **Tests multiple combinations** of hyperparameters.\n",
        "- Uses **cross-validation** to find the **best parameters**.\n",
        "- Prevents **overfitting** and **improves generalization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps in the Code:**\n",
        "1Ô∏è‚É£ Load the **Iris dataset** and split it into **training/testing sets**.\n",
        "2Ô∏è‚É£ Define a **Decision Tree Classifier**.\n",
        "3Ô∏è‚É£ Set up a **hyperparameter grid** for `max_depth` and `min_samples_split`.\n",
        "4Ô∏è‚É£ Use **GridSearchCV** to find the best combination.\n",
        "5Ô∏è‚É£ Train the best model and evaluate its accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Python Code:**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data    # Features\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],  # Test different depths\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_clf = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Best Model Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output:**\n",
        "üîπ **Best hyperparameters** (varies depending on dataset split):\n",
        "```\n",
        "Best Hyperparameters: {'max_depth': 5, 'min_samples_split': 2}\n",
        "Best Model Accuracy: 0.9667\n",
        "```\n",
        "üîπ This means the best decision tree:\n",
        "- Has a **max_depth of 5**.\n",
        "- Requires **at least 2 samples** to split a node.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways:**\n",
        "‚úÖ **GridSearchCV automates hyperparameter tuning**.\n",
        "‚úÖ **Cross-validation ensures reliable results**.\n",
        "‚úÖ Choosing the right `max_depth` & `min_samples_split` **improves accuracy** and prevents **overfitting**.\n"
      ],
      "metadata": {
        "id": "LynwAFIoEVur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jO0Tbka_EVxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJaW2pZdEV00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dY2fQrjfEV3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}