{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcdIwcaCdPVV"
      },
      "outputs": [],
      "source": [
        "#SVM & Naive Bayes                                 Theoretical\n",
        "\n",
        "#1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer: Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks.\n",
        " It finds an optimal hyperplane that best separates different classes in the dataset. The hyperplane is chosen in such a way that it maximizes the margin between different classes, making SVM effective in high-dimensional spaces.\n",
        "\n",
        "#2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Answer:Hard Margin SVM: It strictly separates classes without allowing any misclassification. It works only when data is linearly separable.\n",
        "\n",
        "Soft Margin SVM: It allows some misclassification by introducing a penalty term (C parameter) to handle overlapping classes. This makes it more robust for real-world noisy datasets.\n",
        "\n",
        "#3. What is the mathematical intuition behind SVM?\n",
        "Answer:SVM aims to find a hyperplane defined by the equation:\n",
        "\n",
        "where  is the weight vector and  is the bias. The goal is to maximize the margin between support vectors (points closest to the hyperplane). This is formulated as an optimization problem:\n",
        "\n",
        "subject to: for all training samples .\n",
        "\n",
        "#4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Answer: Lagrange multipliers are used to convert the constrained optimization problem into an unconstrained one using the Lagrangian function. They help in solving the dual form of SVM, allowing the use of kernel functions to handle non-linearly separable data.\n",
        "\n",
        "#5. What are Support Vectors in SVM?\n",
        "\n",
        "Answer: Support vectors are the data points that lie closest to the decision boundary. They determine the optimal position of the hyperplane and have a direct impact on model performance.\n",
        "\n",
        "#6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "Answer: Support Vector Classifier (SVC) is the classification version of SVM, which assigns labels to data points by finding the best possible decision boundary.\n",
        "\n",
        "#7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "Answer: Support Vector Regressor (SVR) is the regression variant of SVM. It attempts to fit the data within a margin of error, known as the -tube, instead of predicting exact values.\n",
        "\n",
        "#8. What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer: The Kernel Trick allows SVM to map data to a higher-dimensional space using kernel functions, enabling it to solve problems where data is not linearly separable.\n",
        "\n",
        "#9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "Answer:Linear Kernel: Used when data is linearly separable.\n",
        "Polynomial Kernel: Maps data to a higher-dimensional space using polynomial transformation.\n",
        "RBF Kernel: Uses Gaussian transformation to model complex relationships.\n",
        "#10. What is the effect of the C parameter in SVM?\n",
        "\n",
        "Answer: The C parameter controls the trade-off between maximizing margin and minimizing classification errors. A higher C results in a stricter boundary with fewer misclassifications, while a lower C allows for a wider margin but may misclassify more points.\n",
        "\n",
        "#11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "Answer:Gamma determines the influence of a single training example. A high gamma leads to a more complex model with tighter decision boundaries, while a low gamma results in a smoother, more generalized model.\n",
        "\n",
        "#12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer: Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem, assuming that features are independent. It is called \"naïve\" because it assumes independence among predictors, which is rarely true in real-world data.\n",
        "\n",
        "#13. What is Bayes’ Theorem?\n",
        "\n",
        "Answer: Bayes’ Theorem states:\n",
        "\n",
        "It calculates the probability of event A occurring given that event B has occurred.\n",
        "\n",
        "#14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "Answer:\n",
        "Gaussian Naïve Bayes: Assumes continuous data follows a normal distribution.\n",
        "\n",
        "Multinomial Naïve Bayes: Used for text classification with word frequency counts.\n",
        "\n",
        "Bernoulli Naïve Bayes: Used for binary features, like presence/absence of a word.\n",
        "\n",
        "#15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "Answer: Gaussian Naïve Bayes is best when features are continuous and follow a normal distribution.\n",
        "\n",
        "#16. What are the key assumptions made by Naïve Bayes?\n",
        "Answer: Features are independent.\n",
        "\n",
        "Every feature contributes equally to the outcome.\n",
        "\n",
        "#17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "Answer:\n",
        "Advantages:\n",
        "\n",
        "Works well with small datasets.\n",
        "\n",
        "Fast and efficient.\n",
        "\n",
        "Handles missing data well.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumption of independence is often unrealistic.\n",
        "\n",
        "Not ideal for complex feature relationships.\n",
        "\n",
        "#18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Answer:\n",
        "It performs well on high-dimensional sparse data, such as text documents, and is computationally efficient.\n",
        "\n",
        "#19. Compare SVM and Naïve Bayes for classification tasks.\n",
        "\n",
        "Answer:\n",
        "\n",
        "SVM: Works well on complex decision boundaries but is computationally expensive.\n",
        "\n",
        "Naïve Bayes: Faster, better for text classification but relies on independence assumption.\n",
        "\n",
        "#20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Answer:\n",
        "Laplace Smoothing prevents zero probability issues by adding a small value to all probability estimates, ensuring unseen events do not get a probability of zero."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                             PRACTICAL"
      ],
      "metadata": {
        "id": "VI-UKWAQdQMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qno 21 : Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "gqsFqDnfdQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Compare accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "UYntYUkAdQSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "svr_model = SVR(kernel='rbf')\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "FwqTj_24dQVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Train SVM with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(\"SVM with Polynomial Kernel\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(svm_poly, X, y)"
      ],
      "metadata": {
        "id": "7Rhb8gzzdQYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "accuracy = nb_model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "yERXQfDsdQbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "categories = ['rec.sport.baseball', 'rec.sport.hockey', 'sci.space', 'comp.graphics']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Create a text classification pipeline\n",
        "model = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(newsgroups_test.data)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "8IcGJL-cdQey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Different C values\n",
        "C_values = [0.01, 1, 100]\n",
        "models = [SVC(kernel='linear', C=C_val).fit(X, y) for C_val in C_values]\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for i, (model, C_val) in enumerate(zip(models, C_values)):\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
        "                         np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    axes[i].contourf(xx, yy, Z, alpha=0.3)\n",
        "    axes[i].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    axes[i].set_title(f\"SVM with C = {C_val}\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S2z5HaZmdQhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features.\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Sample binary dataset\n",
        "X = np.random.randint(0, 2, size=(100, 5))\n",
        "y = np.random.randint(0, 2, size=100)\n",
        "\n",
        "# Train Bernoulli Naïve Bayes model\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "\n",
        "# Predict and evaluate\n",
        "accuracy = bnb.score(X, y)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Ium12NjidQkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Train SVM without feature scaling\n",
        "svm_no_scaling = SVC(kernel='rbf')\n",
        "svm_no_scaling.fit(X_train, y_train)\n",
        "accuracy_no_scaling = svm_no_scaling.score(X_test, y_test)\n",
        "\n",
        "# Train SVM with feature scaling\n",
        "svm_with_scaling = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
        "svm_with_scaling.fit(X_train, y_train)\n",
        "accuracy_with_scaling = svm_with_scaling.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_with_scaling:.2f}\")"
      ],
      "metadata": {
        "id": "VoCFfbhWdQnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing.\n",
        "\n",
        "# Train Gaussian Naïve Bayes without Laplace Smoothing\n",
        "nb_no_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "nb_no_smoothing.fit(X_train, y_train)\n",
        "accuracy_no_smoothing = nb_no_smoothing.score(X_test, y_test)\n",
        "\n",
        "# Train Gaussian Naïve Bayes with Laplace Smoothing\n",
        "nb_with_smoothing = GaussianNB(var_smoothing=1e-2)\n",
        "nb_with_smoothing.fit(X_train, y_train)\n",
        "accuracy_with_smoothing = nb_with_smoothing.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.2f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.2f}\")"
      ],
      "metadata": {
        "id": "M8GzFCDXdQqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel).\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "# Perform GridSearch\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "ti4Sss7TdQtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting to improve accuracy.\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X_imbalanced, y_imbalanced = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Train SVM without class weighting\n",
        "svm_no_weighting = SVC(kernel='linear')\n",
        "svm_no_weighting.fit(X_imbalanced, y_imbalanced)\n",
        "accuracy_no_weighting = svm_no_weighting.score(X_imbalanced, y_imbalanced)\n",
        "\n",
        "# Train SVM with class weighting\n",
        "svm_with_weighting = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_with_weighting.fit(X_imbalanced, y_imbalanced)\n",
        "accuracy_with_weighting = svm_with_weighting.score(X_imbalanced, y_imbalanced)\n",
        "\n",
        "print(f\"Accuracy without Class Weighting: {accuracy_no_weighting:.2f}\")\n",
        "print(f\"Accuracy with Class Weighting: {accuracy_with_weighting:.2f}\")"
      ],
      "metadata": {
        "id": "w9C9dnKkdQwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample dataset (email texts and labels)\n",
        "emails = [\"Win a free iPhone now!\", \"Limited time offer, claim your prize\", \"Meeting at 10 AM\", \"Your invoice is attached\"]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = not spam\n",
        "\n",
        "# Train Multinomial Naïve Bayes classifier\n",
        "spam_model = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())\n",
        "spam_model.fit(emails, labels)\n",
        "\n",
        "# Predict on new emails\n",
        "new_emails = [\"Exclusive deal, win big!\", \"Let's meet tomorrow\"]\n",
        "predictions = spam_model.predict(new_emails)\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "MJyw6JYjdQzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_accuracy = svm_model.score(X_test, y_test)\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_accuracy = nb_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n",
        "print(f\"Naïve Bayes Accuracy: {nb_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Sk_xbacsdQ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results.\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Select top 10 best features\n",
        "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_new, y_train)\n",
        "accuracy = nb_model.score(X_test[:, :10], y_test)\n",
        "\n",
        "print(f\"Accuracy after feature selection: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "mXGv4tDHdQ5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy.\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "# Train One-vs-Rest SVM\n",
        "ovr_model = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_model.fit(X_train, y_train)\n",
        "ovr_accuracy = ovr_model.score(X_test, y_test)\n",
        "\n",
        "# Train One-vs-One SVM\n",
        "ovo_model = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "ovo_accuracy = ovo_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"OvR Accuracy: {ovr_accuracy:.2f}\")\n",
        "print(f\"OvO Accuracy: {ovo_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "CMAtMFyddQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy.\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    print(f\"Kernel: {kernel}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "wNAsg366dQ_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "svm_model = SVC(kernel='linear')\n",
        "scores = cross_val_score(svm_model, X, y, cv=cv)\n",
        "\n",
        "print(f\"Average Accuracy: {scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "aKQz2GMFdRCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance.\n",
        "\n",
        "# Train Naïve Bayes with custom priors\n",
        "nb_prior = GaussianNB(priors=[0.3, 0.7])\n",
        "nb_prior.fit(X_train, y_train)\n",
        "accuracy_prior = nb_prior.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with Custom Priors: {accuracy_prior:.2f}\")"
      ],
      "metadata": {
        "id": "a07iHyOYdRFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy.\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Perform Recursive Feature Elimination\n",
        "rfe = RFE(SVC(kernel='linear'), n_features_to_select=5)\n",
        "X_new = rfe.fit_transform(X, y)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_model.fit(X_new, y)\n",
        "print(\"Accuracy after feature selection:\", svm_model.score(X_new, y))"
      ],
      "metadata": {
        "id": "CekavTqTdRI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "hzPpFlCjdRL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Compute Log Loss\n",
        "log_loss_value = log_loss(y_test, y_prob)\n",
        "print(f\"Log Loss: {log_loss_value:.2f}\")"
      ],
      "metadata": {
        "id": "d-qfyck5dRO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "COR_M4gKdRR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Compute MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "cOz3XpdpdRVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "YbzxQ3vidRX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vahZXL1IdRbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCN6b-lkdReW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UScffgOdRhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXLmWyagdRka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iQ5NL7EdRnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbTxxJdedRqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_Z4pQtodRty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuwUuvzmdRw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fp9FDPhkdR0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3Kw4qPxdR3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMGLmq7qdR6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQC4x0FbdR9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyzZrd0BdSAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSJNkOahdSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5rHqag6dSGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6T33I9AgdSKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Q1MlPHKdSNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eyd2iW6fdSQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "go9vU4sCdSTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWQT0bNIdSWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6uZ0ntmdSZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAe6sd9WdSck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHpAc-HMdSfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbhfJa6GdSip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeiBopW3dSln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0IBpzAhBdSo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_8BdhVxdSsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1xylmWGdSu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfxowQ1tdSx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXIE0OG5dS1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aa_U4IUXdS3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaRsD4X3dS6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq-RQHGDdS9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqqF9hc8dTAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhSnCH9EdTD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y20x6g0kdTGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2Zw2PJrdTKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}